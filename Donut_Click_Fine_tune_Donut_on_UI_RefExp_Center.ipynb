{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ivelin/donut_ui_refexp/blob/main/Donut_Click_Fine_tune_Donut_on_UI_RefExp_Center.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNMqJ821yNVo"
   },
   "source": [
    "# Fine-tune Donut 🍩 on UI RefExp - Center Mass/Click Point Prediction\n",
    "\n",
    "> _NOTE_: This notebook is based on the [Fine-tuning Donut for RefExp notebook](https://github.com/ivelin/donut_ui_refexp/blob/main/Fine_tune_Donut_on_UI_RefExp.ipynb). \n",
    "\n",
    "In this notebook, we'll fine-tune [Donut](https://huggingface.co/docs/transformers/model_doc/donut) on predicting the __center point__ of a UI component based on a natural language Referring Expression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title load local env vars such as secret API tokens\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "WANDB_TOKEN=os.environ.get('WANDB_TOKEN')\n",
    "HUGGING_FACE_HUB_TOKEN=os.environ.get('HUGGING_FACE_HUB_TOKEN')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ot1nP9YHz8co",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Let's install required dependencies\n",
    "\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OqcGNPJHyOlt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q datasets sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zMZ6tiMB1JxD",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q pytorch-lightning wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1kf0n5wkudMT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Let's see what system resources we are running on\n",
    "import os\n",
    "print(f\"CPU Count: {os.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "714_cbKavUrT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7ui-jfWx4TZ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def cpu():\n",
    "    return torch.device('cpu')\n",
    "def gpu(i=0):\n",
    "    return torch.device(f'cuda:{i}')\n",
    "cpu(), gpu(), gpu(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VQ1tIEgDyCJb"
   },
   "outputs": [],
   "source": [
    "\n",
    "def num_gpus():\n",
    "    return torch.cuda.device_count()\n",
    "num_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TvERoforv2qS"
   },
   "outputs": [],
   "source": [
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PdgBa6qK3nup"
   },
   "outputs": [],
   "source": [
    "#@title Login to HuggingFace hub so we can save our trained model checkpoints\n",
    "# !huggingface-cli login\n",
    "import huggingface_hub\n",
    "huggingface_hub.login(HUGGING_FACE_HUB_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GWoh-cGbnWpM"
   },
   "outputs": [],
   "source": [
    "#@title Login to Weights&Biases so we can log and chart our training metrics\n",
    "import wandb\n",
    "\n",
    "wandb.login(key=WANDB_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWYic8VNyDNU"
   },
   "source": [
    "## Load dataset\n",
    "\n",
    "Next, let's load the dataset from the [hub](https://huggingface.co/datasets/ivelin/rico_refexp_combined). We're prepared a RICO based dataset with combined synthetic and crowdsourced UI referring expressions. The first 15K samples in the dataset are crowdsourced. The rest of the 350K samples are synthetically generated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5hU27XC2yEot"
   },
   "outputs": [],
   "source": [
    "REFEXP_DATASET_NAME = \"ivelin/ui_refexp_saved\"\n",
    "\n",
    "# Pick which pretrained checkpoint to start the fine tuning process from\n",
    "REFEXP_MODEL_CHECKPOINT = \"ivelin/donut-refexp-click\"\n",
    "REFEXP_MODEL_CP_BRANCH = 'main' \n",
    "# revision: '348ddad8e958d370b7e341acd6050330faa0500f' # Iou = 0.47\n",
    "# revision: '41210d7c42a22e77711711ec45508a6b63ec380f'# : IoU=0.42 # \n",
    "# use 'main' for latest revision\n",
    "\n",
    "# REFEXP_MODEL_CHECKPOINT = \"ivelin/donut-refexp-draft\"\n",
    "# REFEXP_MODEL_CHECKPOINT = \"naver-clova-ix/donut-base\"\n",
    "# REFEXP_MODEL_CHECKPOINT = \"ivelin/donut-docvqa-demo\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hSguzMVA-KCj"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(REFEXP_DATASET_NAME, num_proc=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjI5uyk48V-g"
   },
   "source": [
    "As can be seen, the dataset contains a training, a validation and a test split. And each example consists of an image, a prompt, and a target bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DYk7tDBy-ys"
   },
   "outputs": [],
   "source": [
    "print(dataset['train'].info)\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6f2fjxGaHWli"
   },
   "outputs": [],
   "source": [
    "#@title Let's look at a sample in the dataset\n",
    "import math\n",
    "import json\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# change this index from 0 to split size to see different samples\n",
    "sample = dataset['train'][1]\n",
    "image = sample['image']\n",
    "width, height = image.size\n",
    "print('sample', sample)\n",
    "print(f\"image width, height: {width, height}\")\n",
    "print(f\"prompt: {sample['prompt']}\")\n",
    "\n",
    "# bb = json.loads(sample[\"target_bounding_box\"])\n",
    "bb = sample[\"target_bounding_box\"]\n",
    "if isinstance(bb, str):\n",
    "  bb = json.loads(bb) \n",
    "\n",
    "print(f\"target bounding box: {bb}\")\n",
    "\n",
    "xmin = math.floor(width*bb[\"xmin\"])\n",
    "ymin = math.floor(height*bb[\"ymin\"])\n",
    "xmax = math.floor(width*bb[\"xmax\"])\n",
    "ymax = math.floor(height*bb[\"ymax\"])\n",
    "\n",
    "print(f\"to image pixel values: xmin, ymin, xmax, ymax: {xmin, ymin, xmax, ymax}\")\n",
    "\n",
    "shape = [(xmin, ymin), (xmax, ymax)]\n",
    "\n",
    "# create rectangle image\n",
    "img1 = ImageDraw.Draw(image)  \n",
    "img1.rectangle(shape, outline =\"green\", width=5)\n",
    "image.thumbnail((960, 1280))\n",
    "width, height = image.size\n",
    "print(f\"resized image width, height: {width, height}\")\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCjMK93Cz3zf"
   },
   "source": [
    "## Load model and processor\n",
    "\n",
    "Next, we load the model (which is an instance of [VisionEncoderDecoderModel](https://huggingface.co/docs/transformers/model_doc/vision-encoder-decoder), and the processor, which is the object that can be used to prepare inputs for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ahkkeo8_o69z"
   },
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderConfig\n",
    "\n",
    "pretrained_repo_name = REFEXP_MODEL_CHECKPOINT\n",
    "pretrained_repo_branch = REFEXP_MODEL_CP_BRANCH\n",
    "\n",
    "# max length of predicted decoder text sequence \n",
    "max_length = 128\n",
    "# normalized input image size (height, width)\n",
    "encoder_image_size = [1280, 960]\n",
    "\n",
    "# update image_size of the encoder\n",
    "# during pre-training, a larger image size was used\n",
    "config = VisionEncoderDecoderConfig.from_pretrained(pretrained_repo_name, branch=pretrained_repo_branch, use_auth_token=HUGGING_FACE_HUB_TOKEN)\n",
    "config.encoder.image_size = encoder_image_size # (height, width)\n",
    "# update max_length of the decoder (for generation)\n",
    "config.decoder.max_length = max_length\n",
    "# TODO we should actually update max_position_embeddings and interpolate the pre-trained ones:\n",
    "# https://github.com/clovaai/donut/blob/0acc65a85d140852b8d9928565f0f6b2d98dc088/donut/model.py#L602"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84TkZP5zz4hE"
   },
   "outputs": [],
   "source": [
    "from transformers import DonutProcessor, VisionEncoderDecoderModel, BartConfig\n",
    "\n",
    "processor = DonutProcessor.from_pretrained(pretrained_repo_name, revision=pretrained_repo_branch, use_auth_token=HUGGING_FACE_HUB_TOKEN)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(pretrained_repo_name, revision=pretrained_repo_branch, config=config, use_auth_token=HUGGING_FACE_HUB_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PfTPbvNRCEDF"
   },
   "source": [
    "## Add special tokens\n",
    "\n",
    "For RefExp, we add special tokens for \\<prompt> \\<target_center>, \\<x>, and \\<y>, to make sure that the model (actually the decoder) learns embedding vectors for those explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CfJMb2o31AA-"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def add_tokens(list_of_tokens: List[str]):\n",
    "    \"\"\"\n",
    "    Add tokens to tokenizer and resize the token embeddings\n",
    "    \"\"\"\n",
    "    newly_added_num = processor.tokenizer.add_tokens(list_of_tokens)\n",
    "    if newly_added_num > 0:\n",
    "        model.decoder.resize_token_embeddings(len(processor.tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b46s3KR-x8Iv"
   },
   "source": [
    "## Create PyTorch dataset\n",
    "\n",
    "Here we create a regular PyTorch dataset.\n",
    "\n",
    "The model doesn't directly take the (image, JSON) pairs as input and labels. Rather, we create `pixel_values`, `decoder_input_ids` and `labels`. These are all PyTorch tensors. The `pixel_values` are the input images (resized, padded and normalized), the `decoder_input_ids` are the decoder inputs, and the `labels` are the decoder targets.\n",
    "\n",
    "The reason we create the `decoder_input_ids` explicitly here is because otherwise, the model would create them automatically based on the `labels` (by prepending the decoder start token ID, replacing -100 tokens by padding tokens). The reason for that is that we don't want the model to learn to generate the entire prompt, which includes the question. Rather, we only want it to learn to generate the answer. Hence, we'll set the labels of the prompt tokens to -100.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_WQUp0m48rd"
   },
   "outputs": [],
   "source": [
    "def validate_bbox(bb):\n",
    "  \"\"\"\n",
    "  Ensures correct coordinates for bounding box. Returns true\n",
    "  Returns\n",
    "  -------\n",
    "  True if bbox coordinates are valid. False otherwise.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    if bb['xmin'] > bb['xmax']:\n",
    "      return False\n",
    "    if bb['ymin'] > bb['ymax']:\n",
    "      return False\n",
    "  except Exception as e:\n",
    "    print(f\"Invalid bbox: {bb}\", e)\n",
    "    return False \n",
    "  return True\n",
    "\n",
    "def bbox_center_point(bb):\n",
    "  if validate_bbox(bb):\n",
    "    # determine the coordinates of the center of each rectangle\n",
    "    bb_x_center = (bb['xmax'] + bb['xmin'])/2\n",
    "    bb_y_center = (bb['ymax'] + bb['ymin'])/2\n",
    "    cp = {'x': bb_x_center, 'y': bb_y_center}\n",
    "  else:\n",
    "    cp = {'x': 0, 'y': 0}\n",
    "  return cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def translate_point_coords_to_tensor_image_size(point=None, input_image_size=None, output_image_size=None):\n",
    "    \"\"\"\n",
    "    Convert relative coordinates to resized tensor image\n",
    "    taking into account padding space.\n",
    "    Args:\n",
    "        original_point: x, y coordinates of the point coordinates in [0..1] range in the original image \n",
    "        input_image_size: (width, height) tuple\n",
    "        output_image_size: (width, height) tuple\n",
    "    \"\"\"    \n",
    "    assert point is not None\n",
    "    assert input_image_size is not None\n",
    "    assert output_image_size is not None\n",
    "    # print(f\"point={point}, input_image_size={input_image_size}, output_image_size={output_image_size}\")\n",
    "    input_width, input_height = input_image_size\n",
    "    output_width, output_height = output_image_size\n",
    "    \n",
    "    ratio = min(output_width/input_width, output_height/input_height)\n",
    "    \n",
    "    resized_height = int(input_height*ratio)\n",
    "    # print(f'>>> resized_height={resized_height}')\n",
    "    resized_width = int(input_width*ratio)\n",
    "    # print(f'>>> resized_width={resized_width}')\n",
    "\n",
    "    if resized_height == input_height and resized_width == input_width:\n",
    "        return\n",
    "\n",
    "    # translation of the relative positioning is only needed for dimentions that have padding\n",
    "    if resized_width < output_width:\n",
    "        # adjust for padding pixels\n",
    "        point['x'] *= (resized_width / output_width)\n",
    "    if resized_height < output_height:\n",
    "        # adjust for padding pixels\n",
    "        point['y'] *= (resized_height / output_height)\n",
    "    # print(f\"translated point={point}, resized_image_size: {resized_width, resized_height}\")\n",
    "        \n",
    "\n",
    "# let's test the translation math above\n",
    "out_size=(960, 1280) # height, width\n",
    "apoint = {'x': 0, 'y': 0}\n",
    "translate_point_coords_to_tensor_image_size(point=apoint, input_image_size=(10,10), output_image_size=out_size)\n",
    "assert apoint['x'] == 0 and apoint['y'] == 0\n",
    "translate_point_coords_to_tensor_image_size(point=apoint, input_image_size=(1000,1000), output_image_size=out_size)\n",
    "assert apoint['x'] == 0 and apoint['y'] == 0\n",
    "apoint = {'x': 1, 'y': 1}\n",
    "in_size=(1000,1000)\n",
    "translate_point_coords_to_tensor_image_size(point=apoint, input_image_size=in_size, output_image_size=out_size)\n",
    "assert apoint['x'] == 1 and apoint['y'] == out_size[0]/out_size[1]\n",
    "apoint = {'x': 1, 'y': 1}\n",
    "translate_point_coords_to_tensor_image_size(point=apoint, input_image_size=(10,10), output_image_size=out_size)\n",
    "assert apoint['x'] == 1 and apoint['y'] == out_size[0] / out_size[1]\n",
    "apoint = {'x': 1, 'y': 1}\n",
    "translate_point_coords_to_tensor_image_size(point=apoint, input_image_size=out_size, output_image_size=out_size)\n",
    "assert apoint['x'] == 1 and apoint['y'] == 1\n",
    "apoint = {'x': 0.5, 'y': 0.5}\n",
    "in_size=(540,960)\n",
    "translate_point_coords_to_tensor_image_size(point=apoint, input_image_size=in_size, output_image_size=out_size)\n",
    "assert apoint['x'] == 0.5 * 720 / out_size[0] and apoint['y'] == 0.5\n",
    "in_size=(1080, 1920)\n",
    "translate_point_coords_to_tensor_image_size(point=apoint, input_image_size=out_size, output_image_size=out_size)\n",
    "assert apoint['x'] == 0.5*720/out_size[0] and apoint['y'] == 0.5\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "7tWX_qJDvw_S"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from typing import Any, List, Tuple\n",
    "import weakref\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "added_tokens = []\n",
    "\n",
    "class DonutDataset(Dataset):\n",
    "    \"\"\"\n",
    "    DonutDataset which is saved in huggingface datasets format. (see details in https://huggingface.co/docs/datasets)\n",
    "    Each row, consists of image blob, prompt and target bounding box.,\n",
    "    and it will be converted into input_tensor(vectorized image) and input_ids(tokenized string).\n",
    "    Args:\n",
    "        dataset_name_or_path: name of dataset (available at huggingface.co/datasets) or the path containing image files and metadata.jsonl\n",
    "        max_length: the max number of tokens for the target sequences\n",
    "        split: whether to load \"train\", \"validation\" or \"test\" split\n",
    "        ignore_id: ignore_index for torch.nn.CrossEntropyLoss\n",
    "        task_start_token: the special token to be fed to the decoder to conduct the target task\n",
    "        prompt_end_token: the special token at the end of the sequences\n",
    "        sort_json_key: whether or not to sort the JSON keys\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_name_or_path: str,\n",
    "        max_length: int,\n",
    "        range_samples: int = None,\n",
    "        shuffle: bool = False,\n",
    "        split: str = \"train\",\n",
    "        ignore_id: int = -100,\n",
    "        task_start_token: str = \"<s>\",\n",
    "        prompt_end_token: str = None,\n",
    "        sort_json_key: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.split = split\n",
    "        self.ignore_id = ignore_id\n",
    "        self.task_start_token = task_start_token\n",
    "        self.prompt_end_token = prompt_end_token if prompt_end_token else task_start_token\n",
    "        self.sort_json_key = sort_json_key\n",
    "\n",
    "        self.dataset = load_dataset(dataset_name_or_path, split=self.split)\n",
    "\n",
    "        self.gt_token_sequences = []\n",
    "        if shuffle:\n",
    "          self.dataset = self.dataset.shuffle()\n",
    "        if range_samples is not None:\n",
    "          self.dataset = self.dataset.select(range_samples)\n",
    "        self.dataset = self.dataset.shuffle()\n",
    "        self.dataset_length = self.dataset.num_rows\n",
    "        # create an in-memory cache for pixel tensors\n",
    "        self.pixel_cache = weakref.WeakValueDictionary()\n",
    "        # create an in-memory cache for input_ids\n",
    "        self.input_ids_cache = weakref.WeakValueDictionary()\n",
    "        for sample in self.dataset:\n",
    "            prompt = sample[\"prompt\"]\n",
    "            # bb = json.loads(sample[\"target_bounding_box\"])\n",
    "            bb = sample[\"target_bounding_box\"]\n",
    "            if isinstance(bb, str):\n",
    "              bb = json.loads(bb)             \n",
    "            cp = bbox_center_point(bb)\n",
    "            # Trim float precision to simplify training with shorter string representations of component coordinates.\n",
    "            # 2 decimals precision seems to be a good balance between component position acccuracy and model convergance time.\n",
    "            # 3 decimals precision is good enough for screenshot size up to [1000x1000], but it takes longer for the model to converge.\n",
    "            # For even finer granurality, we cam increase precision to 4 for [10,000 x 10,000] screen sizes, but it will take much more training time and compute resources to converge.\n",
    "            for key, value in cp.items():\n",
    "              cp[key] = round(value,2)\n",
    "\n",
    "            \n",
    "            # convert relative coordinates to resized tensor image\n",
    "            # taking into account padding space\n",
    "            translate_point_coords_to_tensor_image_size(point=cp, \n",
    "                                                        input_image_size=sample['image'].size, \n",
    "                                                        output_image_size=(processor.image_processor.size[0],processor.image_processor.size[1]))\n",
    "            \n",
    "            assert isinstance(cp, dict)\n",
    "            ground_truth = {\"prompt\": prompt, \"target_center\": cp}\n",
    "            gt_json = ground_truth\n",
    "\n",
    "            j2t = self.json2token(\n",
    "                  gt_json,\n",
    "                  update_special_tokens_for_json_key=self.split == \"train\",\n",
    "                  sort_json_key=self.sort_json_key,\n",
    "              ) + processor.tokenizer.eos_token\n",
    "            self.gt_token_sequences.append(j2t)\n",
    "\n",
    "        self.add_tokens([self.task_start_token, self.prompt_end_token])\n",
    "        self.prompt_end_token_id = processor.tokenizer.convert_tokens_to_ids(self.prompt_end_token)\n",
    "\n",
    "    def json2token(self, obj: Any, update_special_tokens_for_json_key: bool = True, sort_json_key: bool = True):\n",
    "        \"\"\"\n",
    "        Convert an ordered JSON object into a token sequence\n",
    "        \"\"\"\n",
    "        if type(obj) == dict:\n",
    "            if len(obj) == 1 and \"text_sequence\" in obj:\n",
    "                return obj[\"text_sequence\"]\n",
    "            else:\n",
    "                output = \"\"\n",
    "                if sort_json_key:\n",
    "                    keys = sorted(obj.keys(), reverse=True)\n",
    "                else:\n",
    "                    keys = obj.keys()\n",
    "                for k in keys:\n",
    "                    if update_special_tokens_for_json_key:\n",
    "                        self.add_tokens([fr\"<s_{k}>\", fr\"</s_{k}>\"])\n",
    "                    output += (\n",
    "                        fr\"<s_{k}>\"\n",
    "                        + self.json2token(obj[k], update_special_tokens_for_json_key, sort_json_key)\n",
    "                        + fr\"</s_{k}>\"\n",
    "                    )\n",
    "                return output\n",
    "        elif type(obj) == list:\n",
    "            return r\"<sep/>\".join(\n",
    "                [self.json2token(item, update_special_tokens_for_json_key, sort_json_key) for item in obj]\n",
    "            )\n",
    "        else:\n",
    "            obj = str(obj)\n",
    "            if f\"<{obj}/>\" in added_tokens:\n",
    "                obj = f\"<{obj}/>\"  # for categorical special tokens\n",
    "            return obj\n",
    "    \n",
    "    def add_tokens(self, list_of_tokens: List[str]):\n",
    "        \"\"\"\n",
    "        Add special tokens to tokenizer and resize the token embeddings of the decoder\n",
    "        \"\"\"\n",
    "        newly_added_num = processor.tokenizer.add_tokens(list_of_tokens)\n",
    "        if newly_added_num > 0:\n",
    "            model.decoder.resize_token_embeddings(len(processor.tokenizer))\n",
    "            added_tokens.extend(list_of_tokens)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.dataset_length - 1\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Load image from image_path of given dataset_path and convert into input_tensor and labels\n",
    "        Convert gt data into input_ids (tokenized string)\n",
    "        Returns:\n",
    "            input_tensor : preprocessed image\n",
    "            input_ids : tokenized gt_data\n",
    "            labels : masked labels (model doesn't need to predict prompt and pad token)\n",
    "        \"\"\"\n",
    "        sample = self.dataset[idx]\n",
    "\n",
    "        # input_tensor\n",
    "        input_tensor = self.pixel_cache.get(idx)\n",
    "        if input_tensor is None:\n",
    "          pixel_values = processor(sample[\"image\"].convert(\"RGB\"), random_padding=self.split == \"train\", return_tensors=\"pt\").pixel_values\n",
    "          input_tensor = pixel_values.squeeze()\n",
    "          self.pixel_cache[idx] = input_tensor\n",
    "        # elif idx % 100 == 0:\n",
    "        #   print(f'{self.split} dataloader pixel_cache hit at index {idx}')\n",
    "\n",
    "        # input_ids\n",
    "        input_ids = self.input_ids_cache.get(idx)\n",
    "        if input_ids is None:\n",
    "          processed_parse = self.gt_token_sequences[idx]\n",
    "          input_ids = processor.tokenizer(\n",
    "              processed_parse,\n",
    "              add_special_tokens=False,\n",
    "              max_length=self.max_length,\n",
    "              padding=\"max_length\",\n",
    "              truncation=True,\n",
    "              return_tensors=\"pt\",\n",
    "          )[\"input_ids\"].squeeze(0)\n",
    "          self.input_ids_cache[idx] = input_ids\n",
    "        # elif idx % 100 == 0:\n",
    "        #   print(f'{self.split} dataloader input_ids cache hit at index {idx}')\n",
    "\n",
    "\n",
    "        if idx % 200 == 0:\n",
    "          print(f\"sameple #{idx}, input_ids: {input_ids}\")\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            labels = input_ids.clone()\n",
    "            labels[\n",
    "                labels == processor.tokenizer.pad_token_id\n",
    "            ] = self.ignore_id  # model doesn't need to predict pad token\n",
    "            labels[\n",
    "                : torch.nonzero(labels == self.prompt_end_token_id).sum() + 1\n",
    "            ] = self.ignore_id  # model doesn't need to predict prompt (for VQA)\n",
    "            return input_tensor, input_ids, labels\n",
    "        else:\n",
    "            prompt_end_index = torch.nonzero(\n",
    "                input_ids == self.prompt_end_token_id\n",
    "            ).sum()  # return prompt end index instead of target output labels\n",
    "            return input_tensor, input_ids, prompt_end_index, processed_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "4_h6nyTm3RN0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    validation: Dataset({\n",
       "        features: ['image', 'image_id', 'image_file_path', 'prompt', 'target_bounding_box'],\n",
       "        num_rows: 471\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['image', 'image_id', 'image_file_path', 'prompt', 'target_bounding_box'],\n",
       "        num_rows: 15624\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'image_id', 'image_file_path', 'prompt', 'target_bounding_box'],\n",
       "        num_rows: 565\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3JpazNkf8CnA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration ivelin--ui_refexp_saved-6916d19d5dad9975\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/ivelin___parquet/ivelin--ui_refexp_saved-6916d19d5dad9975/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    }
   ],
   "source": [
    "# we update some settings which differ from pretraining; namely the size of the images + no rotation required\n",
    "# source: https://github.com/clovaai/donut/blob/master/config/train_cord.yaml\n",
    "processor.image_processor.size = encoder_image_size[::-1] # should be (width, height)\n",
    "processor.image_processor.do_align_long_axis = False\n",
    "# do not resize in a way that may crop the original image\n",
    "processor.image_processor.do_resize = False\n",
    "# do resize in a way that will fit the original image entirely without cropping out any section of it\n",
    "processor.image_processor.do_thumbnail = True\n",
    "\n",
    "\n",
    "# For warm up phase, consider picking only a small subset to see if the model converges on the data\n",
    "max_train_samples = dataset['train'].num_rows # 15624\n",
    "# pick a range for sampling\n",
    "# range_train_samples = range(4000, 4000+max_train_samples)\n",
    "range_train_samples = range(max_train_samples)\n",
    "\n",
    "train_dataset = DonutDataset(REFEXP_DATASET_NAME, max_length=max_length, \n",
    "                             range_samples=range_train_samples,\n",
    "                             shuffle=True,\n",
    "                             split=\"train\", task_start_token=\"<s_refexp>\", prompt_end_token=\"<s_target_center>\",\n",
    "                             sort_json_key=False,\n",
    "                             )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vlrKXSzLBAwi"
   },
   "outputs": [],
   "source": [
    "train_dataset[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cxv4RS8i-rsJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "# pick a small subset for initial val set to see if validation metrics improve\n",
    "max_val_samples = dataset['validation'].num_rows # 471\n",
    "range_val_samples = range(max_val_samples)\n",
    "# range_val_samples = range(0,max_val_samples,8)\n",
    "\n",
    "val_dataset = DonutDataset(REFEXP_DATASET_NAME, max_length=max_length, range_samples=range_val_samples,\n",
    "                             split=\"validation\", task_start_token=\"<s_refexp>\", prompt_end_token=\"<s_target_center>\",\n",
    "                             sort_json_key=False,\n",
    "                             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ZUvmOdAs7xk"
   },
   "outputs": [],
   "source": [
    "pixel_values, decoder_input_ids, labels = train_dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQMuNYnA4XYk"
   },
   "outputs": [],
   "source": [
    "pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vWKlLJML4o-6"
   },
   "outputs": [],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zud4yPeN4qQb"
   },
   "outputs": [],
   "source": [
    "for decoder_input_id, label in zip(decoder_input_ids.tolist()[:-1], labels.tolist()[1:]):\n",
    "  if label != -100:\n",
    "    print(processor.decode([decoder_input_id]), processor.decode([label]))\n",
    "  else:\n",
    "    print(processor.decode([decoder_input_id]), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0xcQqFDsBmPq"
   },
   "outputs": [],
   "source": [
    "pixel_values, decoder_input_ids, prompt_end_index, processed_parse = val_dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Szz2rquaBq89"
   },
   "outputs": [],
   "source": [
    "pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mUwVF9yBr_u"
   },
   "outputs": [],
   "source": [
    "prompt_end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cj2gybmeBuvQ"
   },
   "outputs": [],
   "source": [
    "processed_parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygTIylugfasG"
   },
   "source": [
    "## Create PyTorch DataLoaders\n",
    "\n",
    "Next, we create corresponding PyTorch DataLoaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nLQ_Vl5MLugu"
   },
   "outputs": [],
   "source": [
    "print(f\"train dataset length: {train_dataset.dataset_length}\")\n",
    "print(f\"validation dataset length: {val_dataset.dataset_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9BFgvT3twpaJ"
   },
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSwRsj8imFuX"
   },
   "outputs": [],
   "source": [
    "#@title Set optimal batch size for training and validation \n",
    "# Currently there is an issue with VisualEncoderDecoder when batch size > 1\n",
    "# Causes error in loss calculation during training\n",
    "train_batch_size = 1 # Usually increments of 8. Value depends on GPU capacity.\n",
    "print(f\"train_batch_size: {train_batch_size}\")\n",
    "val_batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pIkar2gaX4Xl"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxtTVgNnfdkD"
   },
   "source": [
    "Let's verify a batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WHurHlLnL8Xm"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "pixel_values, decoder_input_ids, labels = batch\n",
    "print(pixel_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vo0TXXDL8oHj"
   },
   "outputs": [],
   "source": [
    "decoder_input_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_GvAiCQkPSf"
   },
   "source": [
    "We can clearly see that we have set the labels of all prompt tokens (which includes the prompt) to -100, to make sure the model doesn't learn to generate them. We only start to have labels starting from the \\<s_target_center> decoder input token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f8ehAwgPZrcc"
   },
   "outputs": [],
   "source": [
    "for decoder_input_id, label in zip(decoder_input_ids[0].tolist()[:-1][:50], labels[0].tolist()[1:][:50]):\n",
    "  if label != -100:\n",
    "    print(processor.decode([decoder_input_id]), processor.decode([label]))\n",
    "  else:\n",
    "    print(processor.decode([decoder_input_id]), label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnmD7rRy2WLI"
   },
   "source": [
    "## Define LightningModule\n",
    "\n",
    "We'll fine-tune the model using [PyTorch Lightning](https://www.pytorchlightning.ai/) here, but note that you can of course also just fine-tune with regular PyTorch, HuggingFace [Accelerate](https://github.com/huggingface/accelerate), the HuggingFace [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer), etc.\n",
    "\n",
    "PyTorch Lightning is pretty convenient to handle things like device placement, mixed precision and logging for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXzLACRy1ZLP"
   },
   "source": [
    "### Defining Evaluation Metric\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWQWMb2fH2V1"
   },
   "source": [
    "\n",
    "#### Distance between target centers\n",
    "\n",
    "An intuitive eval metric is to measure the distance in center coordinates between prediction and ground truth.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YYk7Y6SoU9Qe"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def validate_point(point):\n",
    "  \"\"\"\n",
    "  Ensures correct coordinates for point.\n",
    "  Returns True if valid, False otherwise.\n",
    "  -------\n",
    "  True if coordinates are valid. False otherwise.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    if float(point['x']) > 1 or float(point['x']) < 0:\n",
    "      return False\n",
    "    if float(point['y']) > 1 or float(point['y']) < 0:\n",
    "      return False\n",
    "  except Exception as e:\n",
    "    print(f\"Invalid point coordinates: {point}\", e)\n",
    "    return False \n",
    "  return True\n",
    "\n",
    "def get_center_distance(point1, point2):\n",
    "    \"\"\"\n",
    "    Calculate the distance between two points.\n",
    "    Best case, distance between centers of predicted and ground truth is 0.\n",
    "    Worst case,  distance will be the larges diagonal ofn the screen: sqrt(1,1).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    point1 : dict\n",
    "        Keys: {'x', 'y'}\n",
    "    point2 : dict\n",
    "        Keys: {'x', 'y'}\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        in [0, sqrt(1+1)]\n",
    "    \"\"\"\n",
    "    # print(f\"get_center_distance point1: {point1}, point2: {point2}\")\n",
    "    best_case = 0.0\n",
    "    worst_case = math.sqrt(1+1) # max diagonal\n",
    "    # make sure prediction and labels are valid\n",
    "    if not validate_point(point1) or not validate_point(point2):\n",
    "      return worst_case\n",
    "\n",
    "    # determine the coordinates of the center of each rectangle\n",
    "    center_dist = math.sqrt((point1['x'] - point2['x'])**2 + (point1['y'] - point2['y'])**2)\n",
    "    # print(f\"get_center_distance center_dist: {center_dist}\")\n",
    "\n",
    "    assert center_dist >= best_case\n",
    "    assert center_dist <= worst_case\n",
    "    return center_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KxW79QQIMc5"
   },
   "source": [
    "### Donut Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRm5i4gWG-sb"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "# from nltk import edit_distance\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities import rank_zero_only\n",
    "\n",
    "\n",
    "class DonutModelPLModule(pl.LightningModule):\n",
    "    def __init__(self, config, processor, model):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "        self.batch_size = train_batch_size\n",
    "        self.learning_rate = self.config.get(\"lr\")        \n",
    "        # self.log_dict(config)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pixel_values, decoder_input_ids, labels = batch\n",
    "        \n",
    "        outputs = self.model(pixel_values,\n",
    "                             decoder_input_ids=decoder_input_ids[:, :-1],\n",
    "                             labels=labels[:, 1:])\n",
    "        loss = outputs.loss\n",
    "        self.log_dict({\"train_loss\": loss}, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def token2point(self, seq: str):\n",
    "        # print(f\"token2point seq: {seq}\")\n",
    "        target_center = self.processor.token2json(seq)\n",
    "        center_point = target_center.get('target_center')\n",
    "        if center_point is None:\n",
    "          # print(f\"target_center json: {target_center} has no s_target_center attribute, seq:{seq}\")\n",
    "          point = {\"x\": 0, \"y\": 0}\n",
    "          return point\n",
    "        # safeguard in case text prediction is missing some coordinates\n",
    "        # or coordinates are not valid numeric values\n",
    "        try:\n",
    "          x = float(center_point.get(\"x\", 0))\n",
    "        except Exception:\n",
    "          x = -1 # invalid cooridates\n",
    "        try:\n",
    "          y = float(center_point.get(\"y\", 0))\n",
    "        except Exception:\n",
    "          y = -1 # invalid coordinates\n",
    "        # replace str with float coords\n",
    "        point = {\"x\": x, \"y\": y}\n",
    "        # print(f\"token2 center_point float: {center_point}\")\n",
    "        # print(f\"token2point point: {point}\")\n",
    "        return point\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataset_idx=0):\n",
    "        pixel_values, decoder_input_ids, prompt_end_idxs, answers = batch\n",
    "        decoder_prompts = pad_sequence(\n",
    "            [input_id[: end_idx + 1] for input_id, end_idx in zip(decoder_input_ids, prompt_end_idxs)],\n",
    "            batch_first=True,\n",
    "        )\n",
    "        \n",
    "        outputs = self.model.generate(pixel_values,\n",
    "                                   decoder_input_ids=decoder_prompts,\n",
    "                                   max_length=max_length,\n",
    "                                   early_stopping=True,\n",
    "                                   pad_token_id=self.processor.tokenizer.pad_token_id,\n",
    "                                   eos_token_id=self.processor.tokenizer.eos_token_id,\n",
    "                                   use_cache=True,\n",
    "                                   num_beams=1,\n",
    "                                   bad_words_ids=[[self.processor.tokenizer.unk_token_id]],\n",
    "                                   return_dict_in_generate=True,)\n",
    "    \n",
    "        predictions = []\n",
    "        for seq in self.processor.tokenizer.batch_decode(outputs.sequences):\n",
    "            seq = seq.replace(self.processor.tokenizer.eos_token, \"\").replace(self.processor.tokenizer.pad_token, \"\")\n",
    "            seq = re.sub(r\"<.*?>\", \"\", seq, count=1).strip()  # remove first task start token\n",
    "            predictions.append(seq)\n",
    "\n",
    "        scores = list()\n",
    "        for pred, answer in zip(predictions, answers):\n",
    "            answer = re.sub(r\"<.*?>\", \"\", answer, count=1)\n",
    "            answer = answer.replace(self.processor.tokenizer.eos_token, \"\")\n",
    "            # print(f\"predictoin seq: {pred}\\n answer seq: {answer}\")\n",
    "            answer_point = self.token2point(answer)\n",
    "            pred_point = self.token2point(pred)\n",
    "            scores.append(get_center_distance(pred_point, answer_point))\n",
    "            if self.config.get(\"verbose\", False) and len(scores) == 1:\n",
    "              print(f\"      Prediction seq: {pred}\")\n",
    "              print(f\"          Answer seq: {answer}\")\n",
    "              print(f\" Prediction point: {pred_point}\")\n",
    "              print(f\"     Answer point: {answer_point}\")\n",
    "              print(f\"Eval score (Center Distance): {scores[0]}\")\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        # I set this to 1 manually\n",
    "        # (previously set to len(self.config.dataset_name_or_paths))\n",
    "        num_of_loaders = 1\n",
    "        if num_of_loaders == 1:\n",
    "            validation_step_outputs = [validation_step_outputs]\n",
    "        assert len(validation_step_outputs) == num_of_loaders\n",
    "        cnt = [0] * num_of_loaders\n",
    "        total_metric = [0] * num_of_loaders\n",
    "        val_metric = [0] * num_of_loaders\n",
    "        for i, results in enumerate(validation_step_outputs):\n",
    "            for scores in results:\n",
    "                cnt[i] += len(scores)\n",
    "                total_metric[i] += np.sum(scores)\n",
    "            val_metric[i] = total_metric[i] / cnt[i]\n",
    "            val_metric_name = f\"val_metric_{i}th_dataset\"\n",
    "            self.log_dict({val_metric_name: val_metric[i]}, sync_dist=True)\n",
    "        self.log_dict({\"val_metric\": np.sum(total_metric) / np.sum(cnt)}, sync_dist=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html?highlight=configure_optimizers#configure-optimizers\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        # https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html\n",
    "        # we use max below, because we want the lr to decrease if IoU stops increasing\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=2) # previously patience=3, 5\n",
    "        # log initial value for val_metric to avoid train error before its calculated\n",
    "        # self.log_dict({\"val_metric\": 0.0}, sync_dist=True)\n",
    "        return  {\n",
    "          \"optimizer\": optimizer,\n",
    "          \"lr_scheduler\": {\n",
    "              \"scheduler\":scheduler,\n",
    "              \"monitor\": \"val_metric\", # track IoU progress\n",
    "              # \"frequency\": \"indicates how often the metric is updated\"\n",
    "              # If \"monitor\" references validation metrics, then \"frequency\" should be set to a\n",
    "              # multiple of \"trainer.check_val_every_n_epoch\".\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKujfvIDlAHo"
   },
   "source": [
    "Next, we instantiate the module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pxNJhCGjKhtR"
   },
   "outputs": [],
   "source": [
    "# Since the whole dataset is too big to train in a single epoch\n",
    "# We will sample a small subset (5%-10%) per loop and train for a few epochs\n",
    "# Then sample again and loop a few more epochs\n",
    "# In effect simulating training on the whole dataset.\n",
    "max_epochs_per_loop=10 # previously at 30 epochs and 1024 training samples\n",
    "print(f\"max_epochs_per_loop: {max_epochs_per_loop}\")\n",
    "\n",
    "num_training_samples_per_epoch=800 # initially 800\n",
    "print(f\"num_training_samples_per_epoch: {num_training_samples_per_epoch}\")\n",
    "\n",
    "# Start at 3e-5 and reduce gradually every few epochs if loss oscilations too high. Use LR scheduler if epochs > 10.\n",
    "# See scheduler docs: https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html\n",
    "learning_rate= 1e-5 # previously , 1e-6, 1e-5\n",
    "print(f\"learning_rate: {learning_rate}\")\n",
    "\n",
    "# Aim for 10%. Examples: 20 = 800/8*2/10, 10%; 300 for 800/8*30/10\n",
    "warmup_steps=(num_training_samples_per_epoch/train_batch_size)*max_epochs_per_loop/10\n",
    "print(f\"warmup_steps: {warmup_steps}\")\n",
    "\n",
    "\n",
    "def getPLModuleConfig():\n",
    "  config = {\"max_epochs\": max_epochs_per_loop, # aim for 30,\n",
    "            \"val_check_interval\": 1.0, # how many times we want to validate during an epoch\n",
    "            \"check_val_every_n_epoch\":1,\n",
    "            \"gradient_clip_val\":1.0,\n",
    "            \"num_training_samples_per_epoch\": num_training_samples_per_epoch,\n",
    "            \"lr\": learning_rate,\n",
    "            \"train_batch_sizes\": [train_batch_size],\n",
    "            \"val_batch_sizes\": [val_batch_size],\n",
    "            # \"seed\":2022,\n",
    "            # \"num_nodes\": 1,\n",
    "            \"warmup_steps\": warmup_steps, \n",
    "            \"result_path\": \"./result\",\n",
    "            \"verbose\": True,\n",
    "            }\n",
    "  print(f'PL Module Config: {config}')\n",
    "  return config\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZoPiDOPKg0o"
   },
   "source": [
    "## Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQhXL0AdWpuk"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "# clear any previously open logging session\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NiK6-vQHKnBy"
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import StochasticWeightAveraging, LearningRateMonitor\n",
    "\n",
    "wabdb_project_name = \"Donut-RefExp-Click\"\n",
    "wandb_logger = WandbLogger(project=wabdb_project_name)\n",
    "\n",
    "# Take advantage of A100 GPU features\n",
    "# https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "\n",
    "def prep_trainer():\n",
    "  global processor, model, trainer, model_module\n",
    "  config = getPLModuleConfig()\n",
    "  model_module = DonutModelPLModule(config, processor, model)\n",
    "  wandb.finish() # flush any open wandb logging session\n",
    "  wandb_logger = WandbLogger(project=wabdb_project_name)\n",
    "  # Take advantage of A100 GPU features\n",
    "  # https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
    "  torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "  # log learning rate changes by optimizer\n",
    "  lr_monitor = LearningRateMonitor()\n",
    "\n",
    "  trainer = pl.Trainer(\n",
    "          fast_dev_run=True,\n",
    "          accelerator=\"auto\",\n",
    "          devices=\"auto\",\n",
    "          auto_scale_batch_size=True,\n",
    "          auto_lr_find=True,\n",
    "          # accelerator=\"gpu\",\n",
    "          # devices=1,\n",
    "          max_epochs=config.get(\"max_epochs\"),\n",
    "          val_check_interval=config.get(\"val_check_interval\"),\n",
    "          check_val_every_n_epoch=config.get(\"check_val_every_n_epoch\"),\n",
    "          gradient_clip_val=config.get(\"gradient_clip_val\"),\n",
    "          precision=16, # we'll use mixed precision\n",
    "          num_sanity_val_steps=0,\n",
    "          logger=wandb_logger,\n",
    "          benchmark=True, # usually speeds up training,\n",
    "          # strategy=\"ddp_notebook\",\n",
    "          # Other effective optimization techniques follow\n",
    "          # https://pytorch-lightning.readthedocs.io/en/stable/advanced/training_tricks.html#accumulate-gradients\n",
    "          # https://pytorch-lightning.readthedocs.io/en/stable/advanced/training_tricks.html#stochastic-weight-averaging\n",
    "          accumulate_grad_batches={0: 32}, # , 3: 4, 6: 8, 9: 4, 12: 2, 15: 1},\n",
    "          callbacks=[lr_monitor, StochasticWeightAveraging(swa_lrs=3e-7)],\n",
    "          enable_checkpointing=False,\n",
    "          # strategy=\"ddp_notebook\",\n",
    "          # callbacks=[lr_callback, checkpoint_callback],\n",
    "  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KNax96JP0CWR"
   },
   "outputs": [],
   "source": [
    "prep_trainer()\n",
    "trainer.fit(model_module) # , ckpt_path=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KuU1meNkN3-x"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xl4AeMl3jmb"
   },
   "source": [
    "## Push to hub and reuse\n",
    "\n",
    "HuggingFace's [hub](https://huggingface.co/) is a nice place to host, version and share machine learning models (and datasets, and demos in the form of [Spaces](https://huggingface.co/spaces)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7X7GV-YE5loA"
   },
   "source": [
    "Pushing to the hub after training is as easy as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gY24Xk8IDtNR"
   },
   "outputs": [],
   "source": [
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "hub_repo_name = \"ivelin/donut-refexp-click\" # REFEXP_MODEL_CHECKPOINT\n",
    "hub_backup_repo_name = \"ivelin/donut-refexp-click-backup\"\n",
    "local_save_dir=\"saved_pretrained\"\n",
    "local_backup_save_dir=\"saved_pretrained_backup\"\n",
    "\n",
    "def save_pretrained():\n",
    "  global model_module, processor, model\n",
    "\n",
    "  # save a copy to local disk storage just in case push to Hub is rejected.\n",
    "  model_module.processor.save_pretrained(local_save_dir) # repo_id=\"ivelin/donut-refexp-combined-v1.1\") #, use_auth_token=\"...\") \n",
    "  model_module.model.save_pretrained(local_save_dir) # backup_repo_name)\n",
    "\n",
    "  # make sure we can load the model back from disk\n",
    "  processor = processor.from_pretrained(local_save_dir)\n",
    "  model = model.from_pretrained(local_save_dir)\n",
    "\n",
    "  # save a backup\n",
    "  model_module.processor.save_pretrained(local_backup_save_dir) # repo_id=\"ivelin/donut-refexp-combined-v1.1\") #, use_auth_token=\"...\") \n",
    "  model_module.model.save_pretrained(local_backup_save_dir) # backup_repo_name)\n",
    "\n",
    "  #\n",
    "  # here we push the processor and model to the hub\n",
    "  # note that you can add `private=True` in case you're using the private hub\n",
    "  # which makes sure the model is only shared with your colleagues\n",
    "  try:\n",
    "    model_module.processor.push_to_hub(repo_id=hub_repo_name, use_auth_token=HUGGING_FACE_HUB_TOKEN) \n",
    "    model_module.model.push_to_hub(repo_id=hub_repo_name, use_auth_token=HUGGING_FACE_HUB_TOKEN)\n",
    "\n",
    "    processor = DonutProcessor.from_pretrained(hub_repo_name)\n",
    "    model = VisionEncoderDecoderModel.from_pretrained(hub_repo_name)\n",
    "\n",
    "    # save a backup in case uploading to the main model fails and corrupts the data\n",
    "    model_module.processor.push_to_hub(repo_id=hub_backup_repo_name, use_auth_token=HUGGING_FACE_HUB_TOKEN) \n",
    "    model_module.model.push_to_hub(repo_id=hub_backup_repo_name, use_auth_token=HUGGING_FACE_HUB_TOKEN)\n",
    "  except Exception as e:\n",
    "    print(\"Error pushing model to hub\", e)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chaFQM0R3mrb"
   },
   "outputs": [],
   "source": [
    "save_pretrained()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsEe8Dt8Fw1m"
   },
   "source": [
    "## Rinse and Repeat\n",
    "\n",
    "Now that we have confidence in the end to end training process, we can setup a continuous loop that runs unattended overnight.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0VuSxzW_F1Rl"
   },
   "outputs": [],
   "source": [
    "#@ Set training on cruise control. Repeat the above cycle starting each pass with a fresh data subset.\n",
    "\n",
    "\n",
    "# learning_rate= 1e-7 # latest recommended by trainer.tune()\n",
    "print(f\"Learning rate set to {learning_rate}\")\n",
    "# There is an issue with VisualEncoderDecoder when batch > 1 during training loss calculation\n",
    "# train_batch_size = 1 \n",
    "print(f\"Train batch size set to {train_batch_size}\")\n",
    "\n",
    "\n",
    "def prep_next_data_subset():\n",
    "  train_dataset = DonutDataset(REFEXP_DATASET_NAME, max_length=max_length, \n",
    "                              range_samples=range_train_samples,\n",
    "                              shuffle=True,\n",
    "                              split=\"train\", task_start_token=\"<s_refexp>\", prompt_end_token=\"<s_target_center>\",\n",
    "                              sort_json_key=False,\n",
    "                              )\n",
    "  val_dataset = DonutDataset(REFEXP_DATASET_NAME, max_length=max_length, range_samples=range_val_samples,\n",
    "                              split=\"validation\", task_start_token=\"<s_refexp>\", prompt_end_token=\"<s_target_center>\",\n",
    "                              sort_json_key=False,\n",
    "                              )\n",
    "  global train_dataloader, val_dataloader, train_batch_size\n",
    "  train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "  val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)  \n",
    "  return train_dataloader, val_dataloader\n",
    "\n",
    "\n",
    "def train_on_data_subset(train_dataloader=None, val_dataloader=None):\n",
    "  global trainer, model_module\n",
    "  trainer.fit(model_module, train_dataloader, val_dataloader) #, ckpt_path=\"last\")\n",
    "  save_pretrained()\n",
    "\n",
    "def tune_hparams():\n",
    "  \"\"\"Use PL Tune the model to discover optimal hyper-parameters. \n",
    "  More details here: https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html?highlight=batch_size#pytorch_lightning.trainer.Trainer.params.auto_scale_batch_size\"\"\"\n",
    "  global trainer, model_module\n",
    "  print(f\"Tuning trainer hyper parameters...\")\n",
    "  trainer.tune(model_module)\n",
    "  print(f\"Recommended batch_size: {model_module.batch_size}\")\n",
    "  print(f\"Recommended learning rate: {model_module.learning_rate}\")\n",
    "  return model_module.batch_size, model_module.learning_rate\n",
    "\n",
    "\n",
    "def overnight_training():\n",
    "  global train_dataset, val_dataset, max_epochs_per_loop\n",
    "  total_train_samples = dataset['train'].num_rows\n",
    "  total_epochs_per_loop = max_epochs_per_loop * int(total_train_samples/max_train_samples)\n",
    "  # aiming for 100 passes of epoch loops over the whole dataset\n",
    "  total_epochs = total_epochs_per_loop * 100\n",
    "  for epoch_count in range(0, total_epochs, max_epochs_per_loop):\n",
    "    epoch_loop_params = {\n",
    "        'global_epoch_count': epoch_count,\n",
    "        'train_batch_size': train_batch_size, \n",
    "        'learning_rate': learning_rate\n",
    "        }\n",
    "    print(f'Starting epoch loop: {epoch_loop_params}.')\n",
    "    wandb.log(epoch_loop_params)\n",
    "    train_dataloader, val_dataloader = prep_next_data_subset()\n",
    "    print(f'Data loaders prepared.')\n",
    "    prep_trainer()\n",
    "    print(f'Trainer prepared.')\n",
    "    # tune hyper parameters every once in a while\n",
    "    # docs: https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html?highlight=batch_size#pytorch_lightning.trainer.Trainer.params.auto_scale_batch_size\n",
    "    # if epoch_count % total_epochs_per_loop == 0:\n",
    "    #   optimal_batch_size, optimal_lr = tune_hparams()\n",
    "    #   global train_batch_size, learning_rate\n",
    "    #   # save to global space vars for interactive debugging\n",
    "    #   # uncomment below after solving the train loss calc error\n",
    "    #   # train_batch_size = optimal_batch_size\n",
    "    #   learning_rate = optimal_lr\n",
    "    #   # reconfigure trainer with optimal values\n",
    "    #   config[\"lr\"] = optimal_lr\n",
    "    #   config[\"train_batch_sizes\"] = [optimal_batch_size]\n",
    "    #   print(f\"Train learning rate set to {optimal_lr}\")\n",
    "    #   print(f\"Train batch size set to {optimal_batch_size}\")\n",
    "    print(f'Starting training epoch loop.')\n",
    "    train_on_data_subset(train_dataloader=train_dataloader, val_dataloader=val_dataloader)\n",
    "    print(f'Ended training epoch loop.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3wlOVLAMGK32"
   },
   "outputs": [],
   "source": [
    "s_target_centerovernight_training()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Bu-ixvEsRAy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IhVQ-stkd0Ym"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9t50qDh-lGMg"
   },
   "source": [
    "# Inference\n",
    "\n",
    "For inference, you can use this [Gradio playground notebook](https://github.com/ivelin/donut_ui_refexp/blob/main/Inference_Playground_Donut_UI_RefExp_Gradio.ipynb) or this [Huggingface playspace](https://huggingface.co/spaces/ivelin/ui-refexp). Also see the Donut [docs](https://huggingface.co/docs/transformers/main/en/model_doc/donut#inference)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJczyEB6Vq4G"
   },
   "source": [
    "# Questions and Contributions\n",
    "\n",
    "For questions and suggestions, please open an Issue in the [github repo](https://github.com/ivelin/donut_ui_refexp). PRs are also most welcome."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "instance_type": "ml.p3.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
