{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fine-tune Donut on UI RefExp.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "86e583718ef747b3a9a764dffe819561": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ca979743ccfe4e17ae1272b095ca422d",
              "IPY_MODEL_f1030ea9c9ff417d9fd8dddb938013d2",
              "IPY_MODEL_4a9d8c377067448c9e315cb723351677"
            ],
            "layout": "IPY_MODEL_a83a8448be594404ad652a792504984e"
          }
        },
        "ca979743ccfe4e17ae1272b095ca422d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb870f14cee347229650f6c9514a0ea8",
            "placeholder": "​",
            "style": "IPY_MODEL_de7f3867a3cf4df39987907d731629af",
            "value": "100%"
          }
        },
        "f1030ea9c9ff417d9fd8dddb938013d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76fb7fe2a63443c0a9910967aadda37c",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_89249ea947d447e59f8b5576cb6ccc27",
            "value": 3
          }
        },
        "4a9d8c377067448c9e315cb723351677": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db129da47bfc49979a182fc4300630e4",
            "placeholder": "​",
            "style": "IPY_MODEL_dbf1bdc825684f47be043e4d1ea08f1c",
            "value": " 3/3 [00:03&lt;00:00,  1.13s/it]"
          }
        },
        "a83a8448be594404ad652a792504984e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb870f14cee347229650f6c9514a0ea8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de7f3867a3cf4df39987907d731629af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76fb7fe2a63443c0a9910967aadda37c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89249ea947d447e59f8b5576cb6ccc27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db129da47bfc49979a182fc4300630e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbf1bdc825684f47be043e4d1ea08f1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivelin/donut_ui_refexp/blob/main/Fine_tune_Donut_on_UI_RefExp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNMqJ821yNVo"
      },
      "source": [
        "# Fine-tune Donut 🍩 on UI RefExp\n",
        "\n",
        "> _NOTE_: This notebook is based on the [Donut fine-tuning notebooks by Niels Rogge](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/Donut). \n",
        "\n",
        "In this notebook, we'll fine-tune [Donut](https://huggingface.co/docs/transformers/model_doc/donut) (which is an instance of [`VisionEncoderDecoderModel`](https://huggingface.co/docs/transformers/model_doc/vision-encoder-decoder)) on a [UI RefExp dataset](https://huggingface.co/datasets/ivelin/ui_refexp_saved), which is a dataset consisting of triplets: (UI screenshot, prompt, and target bounding box). This way, the model will learn to look at a screenshot image, and answer a prompt referring to a UI component. For example: \"select the search icon next to the menu drawer\". This could be useful for tasks such as converting natural language app documentation to exectuable tests, bug reporting front end test automation and app support chat bots.\n",
        "\n",
        "Multiple specialized models have been proposed to solve the UI RefExp task in recent years: [seq2act](https://paperswithcode.com/paper/mapping-natural-language-instructions-to), [UIBert](https://paperswithcode.com/paper/uibert-learning-generic-multimodal), [pix2struct](https://paperswithcode.com/paper/pix2struct-screenshot-parsing-as-pretraining) and others.\n",
        "\n",
        "Here we will use Donut - a Document Understanding model with state of the art performance as of 2022. We will see Donut can be fine tuned to perform well on the UI RefExp task even though it was not originally designed for it. The intuition is that paper documents have similar visual and language comprehension challenges as UI screens. Since Donut is OCR-free multi-modal model, it should be able to pick up visual and text features in UI components as well as spacial relationships between them.\n",
        "\n",
        "We will repurpose the DocVQA fine-tuning task so that the model learns to output bounding box coordinates of the referred component instead of its text label.\n",
        "\n",
        "Let's get started!\n",
        "\n",
        "## Set-up environment\n",
        "\n",
        "First, let's install the relevant libraries:\n",
        "* 🤗 Transformers, for the model\n",
        "* 🤗 Datasets, for loading + processing the data\n",
        "* PyTorch Lightning, for training the model\n",
        "* Weights and Biases, for logging metrics during training\n",
        "* Sentencepiece, used for tokenization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ot1nP9YHz8co",
        "outputId": "cd11be74-d50d-4ca9-ee4b-f054f70887c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OqcGNPJHyOlt"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zMZ6tiMB1JxD"
      },
      "outputs": [],
      "source": [
        "!pip install -q pytorch-lightning wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Login to HuggingFace hub so we can save our trained model checkpoints\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdgBa6qK3nup",
        "outputId": "5e2c4590-457f-494c-d7a1-a8bba03b24d6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` now requires a token generated from https://huggingface.co/settings/tokens .\n",
            "    \n",
            "Token: \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid.\n",
            "Your token has been saved to /root/.huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Login to Weights&Biases so we can log and chart our training metrics\n",
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWoh-cGbnWpM",
        "outputId": "75c4aa83-5c1b-4920-ca12-fc4f6ff491fc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mivelin-eth\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWYic8VNyDNU"
      },
      "source": [
        "## Load dataset\n",
        "\n",
        "Next, let's load the dataset from the [hub](https://huggingface.co/datasets/naver-clova-ix/cord-v2). We're prepared a minimal dataset for DocVQA, the notebook for that can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Donut/DocVQA/Creating_a_toy_DocVQA_dataset_for_Donut.ipynb).\n",
        "\n",
        "Important here is that we've added a \"ground_truth\" column, containing the ground truth JSON which the model will learn to generate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5hU27XC2yEot"
      },
      "outputs": [],
      "source": [
        "REFEXP_DATASET_NAME = \"ivelin/rico_refexp_combined\"\n",
        "\n",
        "# Pick which pretrained checkpoint to start the fine tuning process from\n",
        "REFEXP_MODEL_CHECKPOINT = \"ivelin/donut-refexp-combined-v1\"\n",
        "\n",
        "# REFEXP_MODEL_CHECKPOINT = \"ivelin/donut-refexp-draft\"\n",
        "# REFEXP_MODEL_CHECKPOINT = \"naver-clova-ix/donut-base\"\n",
        "# REFEXP_MODEL_CHECKPOINT = \"ivelin/donut-docvqa-demo\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(REFEXP_DATASET_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "86e583718ef747b3a9a764dffe819561",
            "ca979743ccfe4e17ae1272b095ca422d",
            "f1030ea9c9ff417d9fd8dddb938013d2",
            "4a9d8c377067448c9e315cb723351677",
            "a83a8448be594404ad652a792504984e",
            "cb870f14cee347229650f6c9514a0ea8",
            "de7f3867a3cf4df39987907d731629af",
            "76fb7fe2a63443c0a9910967aadda37c",
            "89249ea947d447e59f8b5576cb6ccc27",
            "db129da47bfc49979a182fc4300630e4",
            "dbf1bdc825684f47be043e4d1ea08f1c"
          ]
        },
        "id": "hSguzMVA-KCj",
        "outputId": "cf5bdad2-8eaa-4ee5-82d1-2a6d91dafc11"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration ivelin--rico_refexp_combined-00b3f39c0a84947d\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/ivelin___parquet/ivelin--rico_refexp_combined-00b3f39c0a84947d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "86e583718ef747b3a9a764dffe819561"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen, the dataset contains a training, a validation and a test split. And each example consists of an image, a prompt, and a target bounding box."
      ],
      "metadata": {
        "id": "wjI5uyk48V-g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DYk7tDBy-ys",
        "outputId": "271b691b-6669-47ca-c7f3-4f93ac97f5a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetInfo(description='', citation='', homepage='', license='', features={'image': Image(decode=True, id=None), 'image_id': Value(dtype='string', id=None), 'prompt': Value(dtype='string', id=None), 'target_bounding_box': {'xmax': Value(dtype='float64', id=None), 'xmin': Value(dtype='float64', id=None), 'ymax': Value(dtype='float64', id=None), 'ymin': Value(dtype='float64', id=None)}}, post_processed=None, supervised_keys=None, task_templates=None, builder_name=None, config_name=None, version=None, splits={'validation': SplitInfo(name='validation', num_bytes=409057163, num_examples=3191, shard_lengths=None, dataset_name='parquet'), 'test': SplitInfo(name='test', num_bytes=456367852, num_examples=3912, shard_lengths=None, dataset_name='parquet'), 'train': SplitInfo(name='train', num_bytes=42129003316, num_examples=390084, shard_lengths=[4590, 4590, 4590, 4590, 9180, 9180, 4590, 9180, 9180, 9180, 4590, 9180, 4590, 9178, 9178, 9178, 9178, 9178, 9178, 4589, 9178, 4589, 4589, 4589, 9178, 9178, 9178, 9178, 9178, 4589, 4589, 9178, 9178, 9178, 4589, 9178, 4589, 4589, 9178, 9178, 9178, 4589, 4589, 9178, 9178, 9178, 4589, 4589, 9178, 4589, 4589, 9178, 9178, 4589], dataset_name='parquet')}, download_checksums={'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/validation-00000-of-00001-6231b5fbb365cd26.parquet': {'num_bytes': 284902161, 'checksum': '84d01518d13227e91714475c87d6ee59452061b11e7ea8f8e33453f421b17b28'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/test-00000-of-00001-09f7223bee6a973d.parquet': {'num_bytes': 318595519, 'checksum': 'b37ba8683d93fac871b173b5133ac869ec153e8e55752236470d958faf6b9e8c'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00000-of-00085-f3461cdd9dc9472d.parquet': {'num_bytes': 433640875, 'checksum': '3ed49ecda2bfae0fd03c3facc6e79d8078b8c452b5922a716442f42b967c9448'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00001-of-00085-5020257913158460.parquet': {'num_bytes': 424785562, 'checksum': '5a4705bb508a1c4dc742385b922c3644be80526e4f83677082f8daa4bd9979a2'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00002-of-00085-a20edafbec0b704f.parquet': {'num_bytes': 434432898, 'checksum': 'd3d37cb1029fc281c79f1ad7fecd8efde44974cd642fa3b5fd6cc1339d1ac946'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00003-of-00085-b81946b76e056cf4.parquet': {'num_bytes': 373085441, 'checksum': 'f82a98d0141d14f0439f02ef77f79df92ffb3d5bf2cb60531e235a3b9783e390'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00004-of-00085-2c7a2f400a85d114.parquet': {'num_bytes': 301731775, 'checksum': 'cfd596c87cf74ca04fd6b9e456030f510562df3b09bb4f7a72c28c9c28d9b720'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00005-of-00085-abefdcdb0b1adbaa.parquet': {'num_bytes': 269861229, 'checksum': '14e8a6834d0d41561c995a5845cb4df1c8e229cfff9a6b83739160a244038b0b'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00006-of-00085-36df8ccf502abdab.parquet': {'num_bytes': 268812418, 'checksum': 'c733a0999889c52802ccf6c264ab9739aa22fc463a52207f37079f9a7f06451f'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00007-of-00085-f2349a4d9c8faac0.parquet': {'num_bytes': 266539933, 'checksum': '71ce9b50a264371fc825b89c677d0544906126403e084eadfd6e8174b0f2260b'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00008-of-00085-e3435f6a9371b2de.parquet': {'num_bytes': 346235984, 'checksum': '65a48150397af761bb39bfb91a7c58e8d65b3d4d39ab292b5b4037c404701c0c'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00009-of-00085-6316245cccc68c06.parquet': {'num_bytes': 299331628, 'checksum': 'b66ab04e63aaca96e55427da74a80afcdbc0599ed34cf532f9d23ef130a36a1c'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00010-of-00085-e29edd7943eea668.parquet': {'num_bytes': 307812745, 'checksum': 'b3ad505fe87f22db98979deba1a3ea65679199ca53a9935573098c3a4926dee3'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00011-of-00085-afb4ae70e43ebf8c.parquet': {'num_bytes': 304277452, 'checksum': 'aecd8699aaeb21312d8221ddd87e6eb63a3e3b5500fe0adf271e3968f4956c66'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00012-of-00085-c46f70504573fdb6.parquet': {'num_bytes': 294968797, 'checksum': 'dd86a0502f1dd4f9a0803feb19ee2ba1a31b37907345154c4d2a14e0d439886f'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00013-of-00085-ff77dd39650811bc.parquet': {'num_bytes': 278419794, 'checksum': 'd0a8cb73a9d5665d413fe5f564378963ee4e496396062afb42d1171039521916'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00014-of-00085-b5cc0d0467e4753a.parquet': {'num_bytes': 301589114, 'checksum': 'fa6b23cfb7485b7fa43fcb0c50ae77a32a6cfa3defd6501e3461985ca6c343cd'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00015-of-00085-f45399d1640a5c7d.parquet': {'num_bytes': 318887087, 'checksum': '4f0b50bca3f7086472bc7dfc4be000cbbefb46f4d079da5ce0be886aec3537d5'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00016-of-00085-f61d343a34f372c3.parquet': {'num_bytes': 298126959, 'checksum': 'ab81a2df25ed8bcfd090cd0610c83200aa603d131d59a6e0b499da26112280d2'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00017-of-00085-4c1614a15d72588f.parquet': {'num_bytes': 329461507, 'checksum': 'c6e615cc395bdf27f05ba8022edb5b917411cee6a9b842a73abc1c1c4868123a'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00018-of-00085-7d712fa6bacf1be6.parquet': {'num_bytes': 329127922, 'checksum': '8ccfb9b0caef362c0614b9816a0bef49e747f0af51697f7a58473732ea9d81d4'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00019-of-00085-c6f5d160ab4aacd0.parquet': {'num_bytes': 309810836, 'checksum': 'b993ed3084727f36d0b79b1f2401d18ff0a54edb8c0abab4071c22ff6856c5f7'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00020-of-00085-aba217cf9aff5a96.parquet': {'num_bytes': 341483955, 'checksum': '9ebe2cb42bcfb7dff5f5c425ef58400e4b0c116186bc94515495f28aeb9e5f3a'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00021-of-00085-d9faf8d93228ee38.parquet': {'num_bytes': 302754967, 'checksum': 'ba61fb859b8c2350ed54ec7b8989e3be824f968455860235d5e0c53c946119c6'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00022-of-00085-44a73a9c9d387e6f.parquet': {'num_bytes': 350627314, 'checksum': 'd7c75c61ac1931cb35976cecd98d093fc2f2486338edc1a81c8f88fcf9c22664'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00023-of-00085-0f9544edaf7b3579.parquet': {'num_bytes': 297499654, 'checksum': '1cf58055ff63091f6690f3bac96e14cefce030484227d90d93eeb6005c41388b'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00024-of-00085-085b60d193066cec.parquet': {'num_bytes': 314512166, 'checksum': '2a8eda792ac009927522cea6203937a02fa1d7c2c883eaeac92c800cb91be810'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00025-of-00085-b118200ea7712571.parquet': {'num_bytes': 313525723, 'checksum': '84bc44afb9cbe54a5a2bac50c6e1e927053c0efe394d6871f105d10ce5992b49'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00026-of-00085-0974085cd10d5212.parquet': {'num_bytes': 297211405, 'checksum': 'b340733a6b8ecceeff284979dbfe40c6b385fd25dcc5ece40ba8fd52af29565b'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00027-of-00085-8e8451c9c7ce84f8.parquet': {'num_bytes': 317699054, 'checksum': 'd0302096daf818919b64da169402b7ba01fa0f198fdce3f647b0068d80b9026a'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00028-of-00085-004e2c0b08e72700.parquet': {'num_bytes': 301972034, 'checksum': '052f310bea8e350090796dc28e7202dc73019055d783308450fac6090468fd4f'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00029-of-00085-f6f383e0af20885b.parquet': {'num_bytes': 272795305, 'checksum': 'eb1f18666d4b7fca19b28759c4d44d84bbf566753f4f6e3c7ede9818921e1db9'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00030-of-00085-2ff75a6dc18e0c2a.parquet': {'num_bytes': 279730174, 'checksum': 'bd42081d6c3ae18f7935dd9b6f44c8fc6b20454bdb614cab8aef11e7fecba2b1'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00031-of-00085-5828e10715c6c6c7.parquet': {'num_bytes': 342958312, 'checksum': '1702ae091ccde66aa5e58d4850cc394437d031dca6c0678787e3d9625c6c9dac'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00032-of-00085-f2dab9ee4a37bd34.parquet': {'num_bytes': 310556011, 'checksum': '84fc3fec83b2830ac984ccb24a628adf815af7d29d7d2cb976a3ddfcbe646274'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00033-of-00085-7c77d4fe34cf1e3b.parquet': {'num_bytes': 298577583, 'checksum': '5467d2486413a5e18d6f106549cda323c4c4f5bf4cb1b7ee8a54c39fe977078b'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00034-of-00085-ee83893ea2e93ed4.parquet': {'num_bytes': 315922851, 'checksum': 'aa5d3dd7a0d23c9d826da8e7372d13bab6115a58abc9cbc6437a0b3a20f92980'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00035-of-00085-7bf2291ff8c8d050.parquet': {'num_bytes': 326038384, 'checksum': '182dee5b9d4ae27f44b555d5afdb9f9264a77613944cc216a0c58497906146c5'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00036-of-00085-70c0155592cbfef4.parquet': {'num_bytes': 307408570, 'checksum': '643e1d4385919ad19e8e7a2ed5bccf890e51c5947a555a29a542e022f20d50ab'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00037-of-00085-9d9274b43c3f1fea.parquet': {'num_bytes': 301471337, 'checksum': '62b9bb8c705913e1b8afb622edf4fe9873ea084c3e775a99d5ef53be23d8414f'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00038-of-00085-119bb86b50c598f8.parquet': {'num_bytes': 316542117, 'checksum': '28b3980bc9996b4c24e5adb3197ce7a956b42a68432745eb8a5366e68943f252'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00039-of-00085-662441692a1d98e5.parquet': {'num_bytes': 309085814, 'checksum': '1b25802384e6de7fbc738321cb2c83522d250f3be5a23670469c3a15c97b26c8'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00040-of-00085-d5e992ce1ffac8d8.parquet': {'num_bytes': 302986894, 'checksum': '45ec2e6bf1fea2ebdde9b5bcd663023763bdff767aaa3899feb55c1aada07a8e'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00041-of-00085-993e9407a78ecd99.parquet': {'num_bytes': 315567316, 'checksum': 'b3cbb5265bd8ad725a891affd37e2f02da9da5762218bbe0cdb2547201850628'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00042-of-00085-3158ac24e0062a4f.parquet': {'num_bytes': 307725892, 'checksum': 'f52627b71236c538fa64915b4df8ebdf8d8b51874091fc4446abcc5b53b51f4a'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00043-of-00085-027de7560b6666f7.parquet': {'num_bytes': 288240343, 'checksum': 'ab650d7421c8ddd9e4dabc595650a91ea60752ed2843f669a7f267d4fde3c237'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00044-of-00085-ed90ca1e63ec8765.parquet': {'num_bytes': 294380475, 'checksum': '7fac3ff07d82589e516564924fd435e5bec9973cbe41b5887d3dccca3d288455'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00045-of-00085-46bd0dc94c0ca4f5.parquet': {'num_bytes': 307441278, 'checksum': '4dcd3f0823c41d026802aa823ec66ece12e627ca25dbc995d08f242a372e53b3'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00046-of-00085-cd675cb2d1026ac9.parquet': {'num_bytes': 317835930, 'checksum': '86f4f3ef3679ac34a20c7f7c287106617b27809f662b497804bf88aa5c24ae76'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00047-of-00085-3ad4b6ff88f9cccb.parquet': {'num_bytes': 320683120, 'checksum': '17ff9bb39eef053db8dec0af06ed07cde8b8ba70b59214e7b6918a9e968fc288'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00048-of-00085-7ba89fba4fa4e54e.parquet': {'num_bytes': 307588463, 'checksum': 'd1c0c2ccbd912f77f7e26b5fc96655375e822ed56d0db9feb69b34ae718251fe'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00049-of-00085-56e8c13ca25be49e.parquet': {'num_bytes': 293076861, 'checksum': 'a27a168a6f8cf0a74d794c37c692ca662b98e5ac8ffddedfe002e7ef87c48d4e'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00050-of-00085-9480a865de2a2a93.parquet': {'num_bytes': 329763947, 'checksum': '1a82477ef42ec45af2352762e4de1463be6f4c8a923a4908ce5d7a3b924a0a85'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00051-of-00085-9ee3b0a1d9d24b94.parquet': {'num_bytes': 308444925, 'checksum': '3677312ab54dca31b1bebb483ee93b9dbbb2dd41d5e10a70d97d273a4b8a8c3a'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00052-of-00085-3176544695ea7ea2.parquet': {'num_bytes': 323710848, 'checksum': '52af942864fc21a2f11a71cd15370a3a099ac47abf707b7a9c23c4a45898ebe2'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00053-of-00085-97e67510fd41a096.parquet': {'num_bytes': 293805829, 'checksum': '1759b697713ee0acd430d400df25c6e3403b182612a3c8acfefd8f56d562754f'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00054-of-00085-bd4c58384a3184f9.parquet': {'num_bytes': 331830595, 'checksum': '8343a7b64213175a7f96425cf705a3a87ede45691870494c715f25b24505300d'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00055-of-00085-4d49517bfec48823.parquet': {'num_bytes': 316817621, 'checksum': '6cdb5341603e6a44c4eef8ad4e1de0c016ce5bf21b3b188ab6ec91ed6a51d8ae'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00056-of-00085-da74c6080357bd65.parquet': {'num_bytes': 305857357, 'checksum': '7107b52bbd023418fcca7978c0d7623b65127c5a20e7ac65c85111b6f54b0e79'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00057-of-00085-8ea752de4522fdf8.parquet': {'num_bytes': 288192616, 'checksum': '5c9cf5b760735b9966903396062af04dffb5694b4865716302647047b6ca81d4'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00058-of-00085-4b7a4d278cb054de.parquet': {'num_bytes': 314691484, 'checksum': '56508cc96dba700319c2262f8f0ec1750dd4720636ec075e6d1c2e27ec17858d'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00059-of-00085-5d32879f1376f1aa.parquet': {'num_bytes': 308072949, 'checksum': '1cb5adf1fad6bc041cd9a34d4775d86959425a2b5082c7a5a2f3d0df1f72f529'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00060-of-00085-f053dcb46aed8ec7.parquet': {'num_bytes': 289703573, 'checksum': '497f0769b34532987d55875e1bfdea0edd1086c6f6db05aab077d93d6ecb7b3e'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00061-of-00085-be2cc647a6c6c3f8.parquet': {'num_bytes': 329809330, 'checksum': '349dc8c2a9b779c8972e52fef0b0b0ac4d857555f38dc2fc84a5224e38be7d77'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00062-of-00085-189a11f9a522fd67.parquet': {'num_bytes': 303422957, 'checksum': 'b83ac2ed2c8b597f5796ebda6bfeae387c6117a1dc496b218c803e94bd769a53'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00063-of-00085-b53b6ea53c606167.parquet': {'num_bytes': 288740583, 'checksum': '3f52e2fcfd80171647dc2c23a6724809109c9719f5590de8bbf5d15ae5c4c93e'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00064-of-00085-8b8ecfc949d19bd0.parquet': {'num_bytes': 289757439, 'checksum': '252e3f30276202ae40289950e6a333d42280cd15d98502b0fac6346b6de0c91a'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00065-of-00085-7e845d8c6a855b21.parquet': {'num_bytes': 289634432, 'checksum': 'c1335063af8c7d3ae14bfbb59d69a8d3712181ba47e2d5903b6ade2e57c32cac'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00066-of-00085-c42b5fbc1e41e2d2.parquet': {'num_bytes': 333352894, 'checksum': '1de7552f5567740f16b29b5b1c72290d78838a1d7b4f99150f3dd0d27fcdf25d'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00067-of-00085-2c5d28877df175d3.parquet': {'num_bytes': 321251738, 'checksum': '327dc688e2706fea633c6c575c0d30e4c43ebddb923c32062429e6d973a2b519'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00068-of-00085-fd669d0b74258fe0.parquet': {'num_bytes': 302634054, 'checksum': '18a3bd91d03327afdf48060884c0faf96175ab5a50a5b56ecfe8d0e06d3fdc44'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00069-of-00085-4ffd8f0e8e2b52a5.parquet': {'num_bytes': 292886221, 'checksum': '3369b8b6009cc0b7c3fc0df7f9b7dec675901552d8e9396752a6cc7edab9403d'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00070-of-00085-39db9f54b44d6d5d.parquet': {'num_bytes': 300465060, 'checksum': '51abd7e86a31df7f94aa3d0e584f089122e887704e6da55ce6e63bf2f23d05a9'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00071-of-00085-54c3fc5af53319f0.parquet': {'num_bytes': 300059144, 'checksum': '06416df5d04e7800b925cb91a28f85da5cb52b82716eaf7b78d22cdcc5164c4a'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00072-of-00085-d4e10642217ea413.parquet': {'num_bytes': 284487577, 'checksum': '51ec74903dff49a637b2f1b7c063eba8fff3876f08b03381d0bfa46540dc15cb'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00073-of-00085-5f8b5091b568771c.parquet': {'num_bytes': 322014304, 'checksum': '627a697789ed3041fa9ee8144bb90a4933263e7af7fe463ec7ebb8e0b9826bd2'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00074-of-00085-91d2bb92aa1db45d.parquet': {'num_bytes': 316728998, 'checksum': '04b36cd77def83038bfd11bcc0e19084fd385b6159c98608ca12d6c581953fff'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00075-of-00085-e13ba5f653a686a1.parquet': {'num_bytes': 332067056, 'checksum': '078fb1ff9551185efb43ea73d56e1944fb493486dbb1c8613ecbcf071eebdc68'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00076-of-00085-299618a0f1263427.parquet': {'num_bytes': 301582242, 'checksum': 'af7a3d11ce8edadd1340a2b0e072c8c9699c722a47904255ccb76b7206748662'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00077-of-00085-22f7f3479a1919c0.parquet': {'num_bytes': 302924005, 'checksum': '700f404c7467ec7c63f7e8ddeeeee4aed4828678221271fb2cd6dac0fa6d2f22'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00078-of-00085-a8e102ed84ab94a1.parquet': {'num_bytes': 359427940, 'checksum': '3f752399828ab4cc0a524e02976870fc7bcc338e8ba86c749ebac0ba1fe1c39a'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00079-of-00085-ac326da56258d9f5.parquet': {'num_bytes': 331201495, 'checksum': '2b43f423d64ae12fb82235b2acc2982f6db1fa5662bfbcd665e1ffc47e27ade8'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00080-of-00085-358b4ea3c0ddd139.parquet': {'num_bytes': 278258727, 'checksum': 'b8a9a1a5ec3a89216ffd1e93f8e408aac80d5b8cf7ef86b3b45359e29ce73a08'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00081-of-00085-fa452125ac2ef07d.parquet': {'num_bytes': 319102965, 'checksum': '73f4a1219aaef382e87d13b855de65318e9c26787d5e9cf2e047b9513525b594'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00082-of-00085-286c5bf9065384d5.parquet': {'num_bytes': 299899585, 'checksum': 'f0c1bbfe986f31ba1514f0ec24fde2267ffcd2cda9052b037694e48190eaf7d6'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00083-of-00085-162dd5c5d7ab8a7d.parquet': {'num_bytes': 305128811, 'checksum': 'a295f5bcb01cb969ebfca70de835e50046dc0f75f6ab93895ab9d37186d8a952'}, 'https://huggingface.co/datasets/ivelin/rico_refexp_combined/resolve/89a10374941b0e80d75ae00ad6ca81d7b67e33ac/data/train-00084-of-00085-b015e42a159bcc0d.parquet': {'num_bytes': 300122796, 'checksum': '73545163da67455f5e7ffe8b2da5150fb21d768521e7e46a7dabf57204e5c845'}}, download_size=27184189035, post_processing_size=None, dataset_size=42994428331, size_in_bytes=70178617366)\n",
            "DatasetDict({\n",
            "    validation: Dataset({\n",
            "        features: ['image', 'image_id', 'prompt', 'target_bounding_box'],\n",
            "        num_rows: 3191\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['image', 'image_id', 'prompt', 'target_bounding_box'],\n",
            "        num_rows: 3912\n",
            "    })\n",
            "    train: Dataset({\n",
            "        features: ['image', 'image_id', 'prompt', 'target_bounding_box'],\n",
            "        num_rows: 390084\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(dataset['train'].info)\n",
        "print(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Let's look at a sample in the dataset\n",
        "import math\n",
        "import json\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "# change this index from 0 to split size to see different samples\n",
        "sample = dataset['train'][49]\n",
        "image = sample['image']\n",
        "width, height = image.size\n",
        "print(f\"image width, height: {width, height}\")\n",
        "print(f\"prompt: {sample['prompt']}\")\n",
        "\n",
        "# bb = json.loads(sample[\"target_bounding_box\"])\n",
        "bb = sample[\"target_bounding_box\"]\n",
        "\n",
        "\n",
        "print(f\"target bounding box: {bb}\")\n",
        "\n",
        "xmin = math.floor(width*bb[\"xmin\"])\n",
        "ymin = math.floor(height*bb[\"ymin\"])\n",
        "xmax = math.floor(width*bb[\"xmax\"])\n",
        "ymax = math.floor(height*bb[\"ymax\"])\n",
        "\n",
        "print(f\"to image pixel values: xmin, ymin, xmax, ymax: {xmin, ymin, xmax, ymax}\")\n",
        "\n",
        "shape = [(xmin, ymin), (xmax, ymax)]\n",
        "\n",
        "# create rectangle image\n",
        "img1 = ImageDraw.Draw(image)  \n",
        "img1.rectangle(shape, outline =\"green\", width=5)\n",
        "image.resize((int(width*0.5), int(height*0.5)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        },
        "id": "6f2fjxGaHWli",
        "outputId": "21056019-c5c6-460c-beca-2d5a1bf166ee"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image width, height: (540, 960)\n",
            "prompt: go to next\n",
            "target bounding box: {'xmax': 1.0, 'xmin': 0.8759258985519409, 'ymax': 0.1041666641831398, 'ymin': 0.03854166716337204}\n",
            "to image pixel values: xmin, ymin, xmax, ymax: (472, 37, 540, 99)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=270x480 at 0x7F9195993F10>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCjMK93Cz3zf"
      },
      "source": [
        "## Load model and processor\n",
        "\n",
        "Next, we load the model (which is an instance of [VisionEncoderDecoderModel](https://huggingface.co/docs/transformers/model_doc/vision-encoder-decoder), and the processor, which is the object that can be used to prepare inputs for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ahkkeo8_o69z"
      },
      "outputs": [],
      "source": [
        "from transformers import VisionEncoderDecoderConfig\n",
        "\n",
        "pretrained_repo_name = REFEXP_MODEL_CHECKPOINT\n",
        "\n",
        "max_length = 128\n",
        "image_size = [1280, 960]\n",
        "\n",
        "# update image_size of the encoder\n",
        "# during pre-training, a larger image size was used\n",
        "config = VisionEncoderDecoderConfig.from_pretrained(pretrained_repo_name)\n",
        "config.encoder.image_size = image_size # (height, width)\n",
        "# update max_length of the decoder (for generation)\n",
        "config.decoder.max_length = max_length\n",
        "# TODO we should actually update max_position_embeddings and interpolate the pre-trained ones:\n",
        "# https://github.com/clovaai/donut/blob/0acc65a85d140852b8d9928565f0f6b2d98dc088/donut/model.py#L602"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "84TkZP5zz4hE"
      },
      "outputs": [],
      "source": [
        "from transformers import DonutProcessor, VisionEncoderDecoderModel, BartConfig\n",
        "\n",
        "processor = DonutProcessor.from_pretrained(pretrained_repo_name)\n",
        "model = VisionEncoderDecoderModel.from_pretrained(pretrained_repo_name, config=config)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add special tokens\n",
        "\n",
        "For DocVQA, we add special tokens for \\<yes> and \\<no/>, to make sure that the model (actually the decoder) learns embedding vectors for those explicitly."
      ],
      "metadata": {
        "id": "PfTPbvNRCEDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "def add_tokens(list_of_tokens: List[str]):\n",
        "    \"\"\"\n",
        "    Add tokens to tokenizer and resize the token embeddings\n",
        "    \"\"\"\n",
        "    newly_added_num = processor.tokenizer.add_tokens(list_of_tokens)\n",
        "    if newly_added_num > 0:\n",
        "        model.decoder.resize_token_embeddings(len(processor.tokenizer))"
      ],
      "metadata": {
        "id": "CfJMb2o31AA-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # TODO: Do we need this for UI RefExp? It came from the DocVQA code\n",
        "# additional_tokens = [\"<yes/>\", \"<no/>\"]\n",
        "\n",
        "# add_tokens(additional_tokens)"
      ],
      "metadata": {
        "id": "_dnEFkj71UE1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b46s3KR-x8Iv"
      },
      "source": [
        "## Create PyTorch dataset\n",
        "\n",
        "Here we create a regular PyTorch dataset.\n",
        "\n",
        "The model doesn't directly take the (image, JSON) pairs as input and labels. Rather, we create `pixel_values`, `decoder_input_ids` and `labels`. These are all PyTorch tensors. The `pixel_values` are the input images (resized, padded and normalized), the `decoder_input_ids` are the decoder inputs, and the `labels` are the decoder targets.\n",
        "\n",
        "The reason we create the `decoder_input_ids` explicitly here is because otherwise, the model would create them automatically based on the `labels` (by prepending the decoder start token ID, replacing -100 tokens by padding tokens). The reason for that is that we don't want the model to learn to generate the entire prompt, which includes the question. Rather, we only want it to learn to generate the answer. Hence, we'll set the labels of the prompt tokens to -100.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7tWX_qJDvw_S"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "from typing import Any, List, Tuple\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "added_tokens = []\n",
        "\n",
        "class DonutDataset(Dataset):\n",
        "    \"\"\"\n",
        "    DonutDataset which is saved in huggingface datasets format. (see details in https://huggingface.co/docs/datasets)\n",
        "    Each row, consists of image blob, prompt and target bounding box.,\n",
        "    and it will be converted into input_tensor(vectorized image) and input_ids(tokenized string).\n",
        "    Args:\n",
        "        dataset_name_or_path: name of dataset (available at huggingface.co/datasets) or the path containing image files and metadata.jsonl\n",
        "        max_length: the max number of tokens for the target sequences\n",
        "        split: whether to load \"train\", \"validation\" or \"test\" split\n",
        "        ignore_id: ignore_index for torch.nn.CrossEntropyLoss\n",
        "        task_start_token: the special token to be fed to the decoder to conduct the target task\n",
        "        prompt_end_token: the special token at the end of the sequences\n",
        "        sort_json_key: whether or not to sort the JSON keys\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset_name_or_path: str,\n",
        "        max_length: int,\n",
        "        range_samples: int = None,\n",
        "        shuffle: bool = False,\n",
        "        split: str = \"train\",\n",
        "        ignore_id: int = -100,\n",
        "        task_start_token: str = \"<s>\",\n",
        "        prompt_end_token: str = None,\n",
        "        sort_json_key: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.max_length = max_length\n",
        "        self.split = split\n",
        "        self.ignore_id = ignore_id\n",
        "        self.task_start_token = task_start_token\n",
        "        self.prompt_end_token = prompt_end_token if prompt_end_token else task_start_token\n",
        "        self.sort_json_key = sort_json_key\n",
        "\n",
        "        self.dataset = load_dataset(dataset_name_or_path, split=self.split)\n",
        "\n",
        "        self.gt_token_sequences = []\n",
        "        if shuffle:\n",
        "          self.dataset = self.dataset.shuffle()\n",
        "        if range_samples is not None:\n",
        "          self.dataset = self.dataset.select(range_samples)\n",
        "        self.dataset = self.dataset.shuffle()\n",
        "        self.dataset_length = self.dataset.num_rows\n",
        "        for sample in self.dataset:\n",
        "            prompt = sample[\"prompt\"]\n",
        "            # bb = json.loads(sample[\"target_bounding_box\"])\n",
        "            bb = sample[\"target_bounding_box\"]\n",
        "            # Trim float precision to simplify training with shorter string representations of component coordinates.\n",
        "            # 2 decimals precision seems to be a good balance between component position acccuracy and model convergance time.\n",
        "            # 3 decimals precision is good enough for screenshot size up to [1000x1000], but it takes longer for the model to converge.\n",
        "            # For even finer granurality, we cam increase precision to 4 for [10,000 x 10,000] screen sizes, but it will take much more training time and compute resources to converge.\n",
        "            for key, value in bb.items():\n",
        "              bb[key] = round(value,2)\n",
        "\n",
        "            assert isinstance(bb, dict)\n",
        "            ground_truth = {\"prompt\": prompt, \"target_bounding_box\": bb}\n",
        "            gt_json = ground_truth\n",
        "\n",
        "            j2t = self.json2token(\n",
        "                  gt_json,\n",
        "                  update_special_tokens_for_json_key=self.split == \"train\",\n",
        "                  sort_json_key=self.sort_json_key,\n",
        "              ) + processor.tokenizer.eos_token\n",
        "            self.gt_token_sequences.append(j2t)\n",
        "\n",
        "        self.add_tokens([self.task_start_token, self.prompt_end_token])\n",
        "        self.prompt_end_token_id = processor.tokenizer.convert_tokens_to_ids(self.prompt_end_token)\n",
        "\n",
        "    def json2token(self, obj: Any, update_special_tokens_for_json_key: bool = True, sort_json_key: bool = True):\n",
        "        \"\"\"\n",
        "        Convert an ordered JSON object into a token sequence\n",
        "        \"\"\"\n",
        "        if type(obj) == dict:\n",
        "            if len(obj) == 1 and \"text_sequence\" in obj:\n",
        "                return obj[\"text_sequence\"]\n",
        "            else:\n",
        "                output = \"\"\n",
        "                if sort_json_key:\n",
        "                    keys = sorted(obj.keys(), reverse=True)\n",
        "                else:\n",
        "                    keys = obj.keys()\n",
        "                for k in keys:\n",
        "                    if update_special_tokens_for_json_key:\n",
        "                        self.add_tokens([fr\"<s_{k}>\", fr\"</s_{k}>\"])\n",
        "                    output += (\n",
        "                        fr\"<s_{k}>\"\n",
        "                        + self.json2token(obj[k], update_special_tokens_for_json_key, sort_json_key)\n",
        "                        + fr\"</s_{k}>\"\n",
        "                    )\n",
        "                return output\n",
        "        elif type(obj) == list:\n",
        "            return r\"<sep/>\".join(\n",
        "                [self.json2token(item, update_special_tokens_for_json_key, sort_json_key) for item in obj]\n",
        "            )\n",
        "        else:\n",
        "            obj = str(obj)\n",
        "            if f\"<{obj}/>\" in added_tokens:\n",
        "                obj = f\"<{obj}/>\"  # for categorical special tokens\n",
        "            return obj\n",
        "    \n",
        "    def add_tokens(self, list_of_tokens: List[str]):\n",
        "        \"\"\"\n",
        "        Add special tokens to tokenizer and resize the token embeddings of the decoder\n",
        "        \"\"\"\n",
        "        newly_added_num = processor.tokenizer.add_tokens(list_of_tokens)\n",
        "        if newly_added_num > 0:\n",
        "            model.decoder.resize_token_embeddings(len(processor.tokenizer))\n",
        "            added_tokens.extend(list_of_tokens)\n",
        "    \n",
        "    def __len__(self) -> int:\n",
        "        return self.dataset_length - 1\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Load image from image_path of given dataset_path and convert into input_tensor and labels\n",
        "        Convert gt data into input_ids (tokenized string)\n",
        "        Returns:\n",
        "            input_tensor : preprocessed image\n",
        "            input_ids : tokenized gt_data\n",
        "            labels : masked labels (model doesn't need to predict prompt and pad token)\n",
        "        \"\"\"\n",
        "        sample = self.dataset[idx]\n",
        "\n",
        "        # input_tensor\n",
        "        pixel_values = processor(sample[\"image\"].convert(\"RGB\"), random_padding=self.split == \"train\", return_tensors=\"pt\").pixel_values\n",
        "        input_tensor = pixel_values.squeeze()\n",
        "\n",
        "        # input_ids\n",
        "        processed_parse = self.gt_token_sequences[idx]\n",
        "        input_ids = processor.tokenizer(\n",
        "            processed_parse,\n",
        "            add_special_tokens=False,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )[\"input_ids\"].squeeze(0)\n",
        "\n",
        "        if idx % 100 == 0:\n",
        "          print(f\"sameple #{idx}, input_ids: {input_ids}\")\n",
        "\n",
        "        if self.split == \"train\":\n",
        "            labels = input_ids.clone()\n",
        "            labels[\n",
        "                labels == processor.tokenizer.pad_token_id\n",
        "            ] = self.ignore_id  # model doesn't need to predict pad token\n",
        "            labels[\n",
        "                : torch.nonzero(labels == self.prompt_end_token_id).sum() + 1\n",
        "            ] = self.ignore_id  # model doesn't need to predict prompt (for VQA)\n",
        "            return input_tensor, input_ids, labels\n",
        "        else:\n",
        "            prompt_end_index = torch.nonzero(\n",
        "                input_ids == self.prompt_end_token_id\n",
        "            ).sum()  # return prompt end index instead of target output labels\n",
        "            return input_tensor, input_ids, prompt_end_index, processed_parse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_h6nyTm3RN0",
        "outputId": "d0048c38-4ac3-4389-a5cb-67c62f16dcc3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    validation: Dataset({\n",
              "        features: ['image', 'image_id', 'prompt', 'target_bounding_box'],\n",
              "        num_rows: 3191\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['image', 'image_id', 'prompt', 'target_bounding_box'],\n",
              "        num_rows: 3912\n",
              "    })\n",
              "    train: Dataset({\n",
              "        features: ['image', 'image_id', 'prompt', 'target_bounding_box'],\n",
              "        num_rows: 390084\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JpazNkf8CnA",
        "outputId": "6fff92b1-0f76-4f31-acc4-c72be9a1b87e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/donut/processing_donut.py:186: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n",
            "  warnings.warn(\n",
            "WARNING:datasets.builder:Using custom data configuration ivelin--rico_refexp_combined-00b3f39c0a84947d\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/ivelin___parquet/ivelin--rico_refexp_combined-00b3f39c0a84947d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
          ]
        }
      ],
      "source": [
        "# we update some settings which differ from pretraining; namely the size of the images + no rotation required\n",
        "# source: https://github.com/clovaai/donut/blob/master/config/train_cord.yaml\n",
        "processor.feature_extractor.size = image_size[::-1] # should be (width, height)\n",
        "processor.feature_extractor.do_align_long_axis = False\n",
        "\n",
        "# For warm up phase, consider picking only a small subset to see if the model converges on the data\n",
        "max_train_samples = 15000\n",
        "# pick a range for sampling\n",
        "# range_train_samples = range(4000, 4000+max_train_samples)\n",
        "range_train_samples = range(max_train_samples)\n",
        "\n",
        "train_dataset = DonutDataset(REFEXP_DATASET_NAME, max_length=max_length, \n",
        "                             range_samples=range_train_samples,\n",
        "                             shuffle=True,\n",
        "                             split=\"train\", task_start_token=\"<s_refexp>\", prompt_end_token=\"<s_target_bounding_box>\",\n",
        "                             sort_json_key=False,\n",
        "                             )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlrKXSzLBAwi",
        "outputId": "0061f982-7db4-49ba-b41c-fc414dee5981"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sameple #0, input_ids: tensor([57527, 18539, 56738, 48941, 45125, 34445, 57528, 57529, 57535, 50891,\n",
            "        39539, 15524, 57536, 57531, 50891, 39539, 20779, 57532, 57537, 50891,\n",
            "        39539,  7505, 57538, 57533, 50891, 39539, 53979, 57534, 57530,     2,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "          ...,\n",
              "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "          [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
              " \n",
              "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "          ...,\n",
              "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "          [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
              " \n",
              "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "          ...,\n",
              "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
              "          [-1., -1., -1.,  ..., -1., -1., -1.]]]),\n",
              " tensor([57527, 18539, 56738, 48941, 45125, 34445, 57528, 57529, 57535, 50891,\n",
              "         39539, 15524, 57536, 57531, 50891, 39539, 20779, 57532, 57537, 50891,\n",
              "         39539,  7505, 57538, 57533, 50891, 39539, 53979, 57534, 57530,     2,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1]),\n",
              " tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 57535, 50891,\n",
              "         39539, 15524, 57536, 57531, 50891, 39539, 20779, 57532, 57537, 50891,\n",
              "         39539,  7505, 57538, 57533, 50891, 39539, 53979, 57534, 57530,     2,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# pick a small subset for initial val set to see if validation metrics improve\n",
        "max_val_samples = 800\n",
        "range_val_samples = range(max_val_samples)\n",
        "\n",
        "val_dataset = DonutDataset(REFEXP_DATASET_NAME, max_length=max_length, range_samples=range_val_samples,\n",
        "                             split=\"validation\", task_start_token=\"<s_refexp>\", prompt_end_token=\"<s_target_bounding_box>\",\n",
        "                             sort_json_key=False,\n",
        "                             )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxv4RS8i-rsJ",
        "outputId": "3bfe8636-f6a7-4769-9800-389cbabf83df"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration ivelin--rico_refexp_combined-00b3f39c0a84947d\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/ivelin___parquet/ivelin--rico_refexp_combined-00b3f39c0a84947d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pixel_values, decoder_input_ids, labels = train_dataset[0]\n"
      ],
      "metadata": {
        "id": "9ZUvmOdAs7xk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08dae704-f581-4913-e574-aa7a08e8e383"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sameple #0, input_ids: tensor([57527, 18539, 56738, 48941, 45125, 34445, 57528, 57529, 57535, 50891,\n",
            "        39539, 15524, 57536, 57531, 50891, 39539, 20779, 57532, 57537, 50891,\n",
            "        39539,  7505, 57538, 57533, 50891, 39539, 53979, 57534, 57530,     2,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pixel_values.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQMuNYnA4XYk",
        "outputId": "02acf214-5853-47f2-f891-c7fa5e4bdf32"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 1280, 960])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWKlLJML4o-6",
        "outputId": "6308f6d5-69a6-43aa-c299-1bb53f7e3f2d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 57535, 50891,\n",
            "        39539, 15524, 57536, 57531, 50891, 39539, 20779, 57532, 57537, 50891,\n",
            "        39539,  7505, 57538, 57533, 50891, 39539, 53979, 57534, 57530,     2,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for decoder_input_id, label in zip(decoder_input_ids.tolist()[:-1], labels.tolist()[1:]):\n",
        "  if label != -100:\n",
        "    print(processor.decode([decoder_input_id]), processor.decode([label]))\n",
        "  else:\n",
        "    print(processor.decode([decoder_input_id]), label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zud4yPeN4qQb",
        "outputId": "cbf02ec1-344a-420f-b719-ed32efab653a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s_prompt> -100\n",
            "click -100\n",
            "on -100\n",
            "the -100\n",
            "lock -100\n",
            "symbol -100\n",
            "</s_prompt> -100\n",
            "<s_target_bounding_box> <s_xmax>\n",
            "<s_xmax> 0\n",
            "0 .\n",
            ". 98\n",
            "98 </s_xmax>\n",
            "</s_xmax> <s_xmin>\n",
            "<s_xmin> 0\n",
            "0 .\n",
            ". 87\n",
            "87 </s_xmin>\n",
            "</s_xmin> <s_ymax>\n",
            "<s_ymax> 0\n",
            "0 .\n",
            ". 42\n",
            "42 </s_ymax>\n",
            "</s_ymax> <s_ymin>\n",
            "<s_ymin> 0\n",
            "0 .\n",
            ". 36\n",
            "36 </s_ymin>\n",
            "</s_ymin> </s_target_bounding_box>\n",
            "</s_target_bounding_box> </s>\n",
            "</s> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pixel_values, decoder_input_ids, prompt_end_index, processed_parse = val_dataset[0]\n"
      ],
      "metadata": {
        "id": "0xcQqFDsBmPq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1930804b-fafb-4347-ae4f-6d37d869325d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sameple #0, input_ids: tensor([57527, 18539, 48941, 37612, 48023, 57528, 57529, 57535, 50891, 39539,\n",
            "        15524, 57536, 57531, 54824, 35934, 57532, 57537, 50891, 39539, 55654,\n",
            "        57538, 57533, 50891, 39539, 55785, 57534, 57530,     2,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pixel_values.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Szz2rquaBq89",
        "outputId": "f9044ad1-6078-4ff2-dc94-d4a74bcd0ad6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 1280, 960])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_end_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mUwVF9yBr_u",
        "outputId": "8d1966d2-6f55-405a-b97e-e80a73a45d8f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_parse"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "cj2gybmeBuvQ",
        "outputId": "76a639e0-a611-4b97-da4b-3650e68746d9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s_prompt>click the activate</s_prompt><s_target_bounding_box><s_xmax>0.98</s_xmax><s_xmin>0.02</s_xmin><s_ymax>0.46</s_ymax><s_ymin>0.39</s_ymin></s_target_bounding_box></s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygTIylugfasG"
      },
      "source": [
        "## Create PyTorch DataLoaders\n",
        "\n",
        "Next, we create corresponding PyTorch DataLoaders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "nLQ_Vl5MLugu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5081318-4130-4a8f-ae9a-84b28f613724"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train dataset length: 15000\n",
            "validation dataset length: 800\n"
          ]
        }
      ],
      "source": [
        "print(f\"train dataset length: {train_dataset.dataset_length}\")\n",
        "print(f\"validation dataset length: {val_dataset.dataset_length}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "id": "pIkar2gaX4Xl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2980c68-f449-433c-f1c2-e868e1571913"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxtTVgNnfdkD"
      },
      "source": [
        "Let's verify a batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHurHlLnL8Xm",
        "outputId": "4bf33af5-91dc-4082-a890-f29761639f81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 1280, 960])\n"
          ]
        }
      ],
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "pixel_values, decoder_input_ids, labels = batch\n",
        "print(pixel_values.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_input_ids.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vo0TXXDL8oHj",
        "outputId": "f1490b6d-0711-435e-ed1d-89c6317b21d7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can clearly see that we have set the labels of all prompt tokens (which includes the prompt) to -100, to make sure the model doesn't learn to generate them. We only start to have labels starting from the \\<s_target_bounding_box> decoder input token."
      ],
      "metadata": {
        "id": "a_GvAiCQkPSf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8ehAwgPZrcc",
        "outputId": "a4fb69f8-7dc6-45e9-fb72-ea18691ef2ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s_prompt> -100\n",
            "open -100\n",
            "we -100\n",
            "all -100\n",
            "know -100\n",
            "item -100\n",
            "</s_prompt> -100\n",
            "<s_target_bounding_box> <s_xmax>\n",
            "<s_xmax> 1.0\n",
            "1.0 </s_xmax>\n",
            "</s_xmax> <s_xmin>\n",
            "<s_xmin> 0.0\n",
            "0.0 </s_xmin>\n",
            "</s_xmin> <s_ymax>\n",
            "<s_ymax> 0.5\n",
            "0.5 </s_ymax>\n",
            "</s_ymax> <s_ymin>\n",
            "<s_ymin> 0\n",
            "0 .\n",
            ". 36\n",
            "36 </s_ymin>\n",
            "</s_ymin> </s_target_bounding_box>\n",
            "</s_target_bounding_box> </s>\n",
            "</s> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n",
            "<pad> -100\n"
          ]
        }
      ],
      "source": [
        "for decoder_input_id, label in zip(decoder_input_ids[0].tolist()[:-1][:50], labels[0].tolist()[1:][:50]):\n",
        "  if label != -100:\n",
        "    print(processor.decode([decoder_input_id]), processor.decode([label]))\n",
        "  else:\n",
        "    print(processor.decode([decoder_input_id]), label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnmD7rRy2WLI"
      },
      "source": [
        "## Define LightningModule\n",
        "\n",
        "We'll fine-tune the model using [PyTorch Lightning](https://www.pytorchlightning.ai/) here, but note that you can of course also just fine-tune with regular PyTorch, HuggingFace [Accelerate](https://github.com/huggingface/accelerate), the HuggingFace [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer), etc.\n",
        "\n",
        "PyTorch Lightning is pretty convenient to handle things like device placement, mixed precision and logging for you."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Evaluation Metric\n",
        "\n",
        "DocVQA normally uses Edit Distance, but that is not the most natural choice for bounding box evaluation.\n",
        "\n",
        "### Distance between rectangle centers\n",
        "\n",
        "In the early stages of training, center distance is a useful coarse grained eval metric. It tells us how close the center of the predicted bounding box is from the center of the ground truth bounding box.\n",
        "\n",
        "As the model improves, we can switch to a more fine grained eval metric such as IoU. See further below.\n"
      ],
      "metadata": {
        "id": "UXzLACRy1ZLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def get_center_distance(bb1, bb2):\n",
        "    \"\"\"\n",
        "    Calculate the distance between the centers of two bounding boxes.\n",
        "    Best case, distance between centers of predicted and ground truth bounding boxes will be 0.\n",
        "    Worst case,  distance will be the larges diagonal in the screen - sqrt(1,1).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    bb1 : dict\n",
        "        Keys: {'xmin', 'xmax', 'ymin', 'ymax'}\n",
        "        The (xmin, ymin) position is at the top left corner,\n",
        "        the (xmax, y2) position is at the bottom right corner\n",
        "    bb2 : dict\n",
        "        Keys: {'xmin', 'xmax', 'ymin', 'ymax'}\n",
        "        The (x, y) position is at the top left corner,\n",
        "        the (xmax, ymax) position is at the bottom right corner\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        in [0, sqrt(1+1)]\n",
        "    \"\"\"\n",
        "    best_case = 0.0\n",
        "    worst_case = math.sqrt(1+1) # max diagonal\n",
        "    # print(f\"IoU input bb1, bb2: {bb1} , {bb2}\")\n",
        "    # if predictions are not resulting in properly shaped bounding boxes, return no-match\n",
        "    try:\n",
        "      if bb1['xmin'] > bb1['xmax']:\n",
        "        return worst_case # max distance = sqrt(1+1)\n",
        "      if bb1['ymin'] > bb1['ymax']:\n",
        "        return worst_case # max distance\n",
        "      # if any of the bounding box labels are not properly shaped, return no-match\n",
        "      if bb2['xmin'] > bb2['xmax']:\n",
        "        return worst_case  # max distance\n",
        "      if bb2['ymin'] > bb2['ymax']:\n",
        "        return worst_case  # max distance\n",
        "    except Exception as e:\n",
        "      print(f\"Error evaluating center distance between {bb1} and {bb2}\", e)\n",
        "      return worst_case      \n",
        "\n",
        "    # determine the coordinates of the center of each rectangle\n",
        "    bb1_x_center = (bb1['xmax'] + bb1['xmin'])/2\n",
        "    bb1_y_center = (bb1['ymax'] + bb1['ymin'])/2\n",
        "\n",
        "    bb2_x_center = (bb2['xmax'] + bb2['xmin'])/2\n",
        "    bb2_y_center = (bb2['ymax'] + bb2['ymin'])/2\n",
        "    center_dist = math.sqrt((bb2_x_center - bb1_x_center)**2 + (bb2_y_center - bb1_y_center)**2)\n",
        "\n",
        "    assert center_dist >= best_case\n",
        "    assert center_dist <= worst_case # sqrt(1+1)\n",
        "    return center_dist"
      ],
      "metadata": {
        "id": "YYk7Y6SoU9Qe"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Intersection over Union\n",
        "\n",
        "We can use [Intersection over Union](https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/) (IoU) to measure bounding box match as a validation progress metric instead of the Edit Distance metric used in DocVQA.\n",
        "\n",
        "Since the model outputs bounding box coordinates, we would like to see these bounding boxes trend towards overlapping exactly with the ground truth.\n",
        "\n",
        "![IoU image](https://i.stack.imgur.com/n1AZj.png)\n",
        "\n",
        "Edit distance also trends towards full match, but it may show less useful intermediate values. For example `xmin=0.123` and `xmin=0.923` are only 1 character separated, but in terms of bounding box overlap, they are very far apart.\n"
      ],
      "metadata": {
        "id": "RKtTK9RWU9kH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_iou(bb1, bb2):\n",
        "    \"\"\"\n",
        "    Calculate the Intersection over Union (IoU) of two bounding boxes.\n",
        "    Best case, IoU is 1 indicating perfect match between prediction and ground truth.\n",
        "    Worst case, IoU is 0 when no overlap between bounding boxes.\n",
        "    Modifed version from the following original on stackoverflow:\n",
        "    https://stackoverflow.com/questions/25349178/calculating-percentage-of-bounding-box-overlap-for-image-detector-evaluation\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    bb1 : dict\n",
        "        Keys: {'xmin', 'xmax', 'ymin', 'ymax'}\n",
        "        The (xmin, ymin) position is at the top left corner,\n",
        "        the (xmax, y2) position is at the bottom right corner\n",
        "    bb2 : dict\n",
        "        Keys: {'xmin', 'xmax', 'ymin', 'ymax'}\n",
        "        The (x, y) position is at the top left corner,\n",
        "        the (xmax, ymax) position is at the bottom right corner\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        in [0, 1]\n",
        "    \"\"\"\n",
        "    best_case = 1.0\n",
        "    worst_case = 0.0\n",
        "    # print(f\"IoU input bb1, bb2: {bb1} , {bb2}\")\n",
        "    # if predictions are not resulting in properly shaped bounding boxes, return no-match\n",
        "    try:\n",
        "      if bb1['xmin'] >= bb1['xmax']:\n",
        "        return worst_case\n",
        "      if bb1['ymin'] >= bb1['ymax']:\n",
        "        return worst_case\n",
        "\n",
        "      # if any of the bounding box labels are not properly shaped, return no-match\n",
        "      if bb2['xmin'] >= bb2['xmax']:\n",
        "        return worst_case\n",
        "      if bb2['ymin'] >= bb2['ymax']:\n",
        "        return worst_case\n",
        "    except Exception as e:\n",
        "      print(f\"Error evaluating IoU between {bb1} and {bb2}\", e)\n",
        "      return worst_case\n",
        "\n",
        "    # determine the coordinates of the intersection rectangle\n",
        "    x_left = max(bb1['xmin'], bb2['xmin'])\n",
        "    y_top = max(bb1['ymin'], bb2['ymin'])\n",
        "    x_right = min(bb1['xmax'], bb2['xmax'])\n",
        "    y_bottom = min(bb1['ymax'], bb2['ymax'])\n",
        "\n",
        "    # print(f\"IoU x_left: {x_left}, y_top: {y_top}, x_right: {x_right}, y_bottom: {y_bottom}\")\n",
        "\n",
        "    if x_right < x_left or y_bottom < y_top:\n",
        "        return worst_case # no bbox overlap\n",
        "\n",
        "    # The intersection of two axis-aligned bounding boxes is always an\n",
        "    # axis-aligned bounding box\n",
        "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
        "    # print(f\"IoU intersection_area: {intersection_area}\")\n",
        "\n",
        "    # compute the area of both AABBs\n",
        "    bb1_area = (bb1['xmax'] - bb1['xmin']) * (bb1['ymax'] - bb1['ymin'])\n",
        "    bb2_area = (bb2['xmax'] - bb2['xmin']) * (bb2['ymax'] - bb2['ymin'])\n",
        "    # print(f\"IoU bb1_area: {bb1_area}\")\n",
        "    # print(f\"IoU bb2_area: {bb2_area}\")\n",
        "\n",
        "    # compute the intersection over union by taking the intersection\n",
        "    # area and dividing it by the sum of prediction + ground-truth\n",
        "    # areas - the interesection area\n",
        "    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n",
        "    # if iou > 0:\n",
        "    #   print(f\"IoU input bb1, bb2: {bb1} , {bb2}\")\n",
        "    #   print(f\"IoU : {iou}\")\n",
        "    assert iou >= worst_case\n",
        "    assert iou <= best_case\n",
        "    return iou"
      ],
      "metadata": {
        "id": "AmDmvkkfnDef"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "oRm5i4gWG-sb"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "# from nltk import edit_distance\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.utilities import rank_zero_only\n",
        "\n",
        "\n",
        "class DonutModelPLModule(pl.LightningModule):\n",
        "    def __init__(self, config, processor, model):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.processor = processor\n",
        "        self.model = model\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        pixel_values, decoder_input_ids, labels = batch\n",
        "        \n",
        "        outputs = self.model(pixel_values,\n",
        "                             decoder_input_ids=decoder_input_ids[:, :-1],\n",
        "                             labels=labels[:, 1:])\n",
        "        loss = outputs.loss\n",
        "        self.log_dict({\"train_loss\": loss}, sync_dist=True)\n",
        "        return loss\n",
        "\n",
        "    def token2bbox(self, seq: str):\n",
        "        target_bbox = self.processor.token2json(seq)\n",
        "        bbox = target_bbox.get('target_bounding_box')\n",
        "        if bbox is None:\n",
        "          print(f\"token2bbox seq has no target_bounding_box, seq:{seq}\")\n",
        "          bbox = bbox = {\"xmin\": 0, \"ymin\": 0, \"xmax\": 0, \"ymax\": 0}\n",
        "          return bbox\n",
        "        # print(f\"token2 bounding box json: {bbox}\")\n",
        "        # safeguard in case text prediction is missing some bounding box coordinates\n",
        "        # or coordinates are not valid numeric values\n",
        "        try:\n",
        "          xmin = float(bbox.get(\"xmin\", 0))\n",
        "        except ValueError:\n",
        "          xmin = 0\n",
        "        try:\n",
        "          ymin = float(bbox.get(\"ymin\", 0))\n",
        "        except ValueError:\n",
        "          ymin = 0\n",
        "        try:\n",
        "          xmax = float(bbox.get(\"xmax\", 1))\n",
        "        except ValueError:\n",
        "          xmax = 1\n",
        "        try:\n",
        "          ymax = float(bbox.get(\"ymax\", 1))\n",
        "        except ValueError:\n",
        "          ymax = 1\n",
        "        # replace str with float coords\n",
        "        bbox = {\"xmin\": xmin, \"ymin\": ymin, \"xmax\": xmax, \"ymax\": ymax}\n",
        "        # print(f\"token2 bounding box float: {bbox}\")\n",
        "        return bbox\n",
        "\n",
        "    def validation_step(self, batch, batch_idx, dataset_idx=0):\n",
        "        pixel_values, decoder_input_ids, prompt_end_idxs, answers = batch\n",
        "        decoder_prompts = pad_sequence(\n",
        "            [input_id[: end_idx + 1] for input_id, end_idx in zip(decoder_input_ids, prompt_end_idxs)],\n",
        "            batch_first=True,\n",
        "        )\n",
        "        \n",
        "        outputs = self.model.generate(pixel_values,\n",
        "                                   decoder_input_ids=decoder_prompts,\n",
        "                                   max_length=max_length,\n",
        "                                   early_stopping=True,\n",
        "                                   pad_token_id=self.processor.tokenizer.pad_token_id,\n",
        "                                   eos_token_id=self.processor.tokenizer.eos_token_id,\n",
        "                                   use_cache=True,\n",
        "                                   num_beams=1,\n",
        "                                   bad_words_ids=[[self.processor.tokenizer.unk_token_id]],\n",
        "                                   return_dict_in_generate=True,)\n",
        "    \n",
        "        predictions = []\n",
        "        for seq in self.processor.tokenizer.batch_decode(outputs.sequences):\n",
        "            seq = seq.replace(self.processor.tokenizer.eos_token, \"\").replace(self.processor.tokenizer.pad_token, \"\")\n",
        "            seq = re.sub(r\"<.*?>\", \"\", seq, count=1).strip()  # remove first task start token\n",
        "            predictions.append(seq)\n",
        "\n",
        "        scores = list()\n",
        "        for pred, answer in zip(predictions, answers):\n",
        "            answer = re.sub(r\"<.*?>\", \"\", answer, count=1)\n",
        "            answer = answer.replace(self.processor.tokenizer.eos_token, \"\")\n",
        "            answer_bbox = self.token2bbox(answer)\n",
        "            pred_bbox = self.token2bbox(pred)\n",
        "            scores.append(get_center_distance(pred_bbox, answer_bbox))\n",
        "            # scores.append(get_iou(pred_bbox, answer_bbox))\n",
        "            if self.config.get(\"verbose\", False) and len(scores) == 1:\n",
        "              print(f\"      Prediction: {pred}\")\n",
        "              print(f\"          Answer: {answer}\")\n",
        "              print(f\" Prediction bbox: {pred_bbox}\")\n",
        "              print(f\"     Answer bbox: {answer_bbox}\")\n",
        "              print(f\"Eval score (Center Distance): {scores[0]}\")\n",
        "              # print(f\"Eval score (IoU): {scores[0]}\")\n",
        "              # print(f\"Eval score (Edit Distance): {scores[2]}\")\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def validation_epoch_end(self, validation_step_outputs):\n",
        "        # I set this to 1 manually\n",
        "        # (previously set to len(self.config.dataset_name_or_paths))\n",
        "        num_of_loaders = 1\n",
        "        if num_of_loaders == 1:\n",
        "            validation_step_outputs = [validation_step_outputs]\n",
        "        assert len(validation_step_outputs) == num_of_loaders\n",
        "        cnt = [0] * num_of_loaders\n",
        "        total_metric = [0] * num_of_loaders\n",
        "        val_metric = [0] * num_of_loaders\n",
        "        for i, results in enumerate(validation_step_outputs):\n",
        "            for scores in results:\n",
        "                cnt[i] += len(scores)\n",
        "                total_metric[i] += np.sum(scores)\n",
        "            val_metric[i] = total_metric[i] / cnt[i]\n",
        "            val_metric_name = f\"val_metric_{i}th_dataset\"\n",
        "            self.log_dict({val_metric_name: val_metric[i]}, sync_dist=True)\n",
        "        self.log_dict({\"val_metric\": np.sum(total_metric) / np.sum(cnt)}, sync_dist=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # TODO add scheduler\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.config.get(\"lr\"))\n",
        "    \n",
        "        return optimizer\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return train_dataloader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return val_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we instantiate the module:"
      ],
      "metadata": {
        "id": "bKujfvIDlAHo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "pxNJhCGjKhtR"
      },
      "outputs": [],
      "source": [
        "config = {\"max_epochs\": 2, # aim for 30,\n",
        "          \"val_check_interval\":0.4, # how many times we want to validate during an epoch\n",
        "          \"check_val_every_n_epoch\":1,\n",
        "          \"gradient_clip_val\":1.0,\n",
        "          \"num_training_samples_per_epoch\": 800,\n",
        "          \"lr\":3e-6, # Start at 3e-5 and reduce gradually every few epochs if loss oscilations too high\n",
        "          \"train_batch_sizes\": [8],\n",
        "          \"val_batch_sizes\": [1],\n",
        "          # \"seed\":2022,\n",
        "          \"num_nodes\": 1,\n",
        "          \"warmup_steps\": 20, # 20 = 800/8*2/10, 10%; 300 for 800/8*30/10, 10%\n",
        "          \"result_path\": \"./result\",\n",
        "          \"verbose\": True,\n",
        "          }\n",
        " \n",
        "model_module = DonutModelPLModule(config, processor, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZoPiDOPKg0o"
      },
      "source": [
        "## Train!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "# clear any previously open logging session\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "GQhXL0AdWpuk"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "NiK6-vQHKnBy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "5ad1f7f7-8201-4fbd-a4ca-05de7b2f24df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mivelin-eth\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>./wandb/run-20230123_155405-dr33kkoh</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/ivelin-eth/Donut-RefExp/runs/dr33kkoh\" target=\"_blank\">zany-music-111</a></strong> to <a href=\"https://wandb.ai/ivelin-eth/Donut-RefExp\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href=\"https://wandb.ai/ivelin-eth/Donut-RefExp\" target=\"_blank\">https://wandb.ai/ivelin-eth/Donut-RefExp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href=\"https://wandb.ai/ivelin-eth/Donut-RefExp/runs/dr33kkoh\" target=\"_blank\">https://wandb.ai/ivelin-eth/Donut-RefExp/runs/dr33kkoh</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:Using 16bit None Automatic Mixed Precision (AMP)\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "from pytorch_lightning.loggers import WandbLogger\n",
        " \n",
        "\n",
        "wandb_logger = WandbLogger(project=\"Donut-RefExp\")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "        accelerator=\"gpu\",\n",
        "        devices=1,\n",
        "        max_epochs=config.get(\"max_epochs\"),\n",
        "        val_check_interval=config.get(\"val_check_interval\"),\n",
        "        check_val_every_n_epoch=config.get(\"check_val_every_n_epoch\"),\n",
        "        gradient_clip_val=config.get(\"gradient_clip_val\"),\n",
        "        precision=16, # we'll use mixed precision\n",
        "        num_sanity_val_steps=0,\n",
        "        logger=wandb_logger\n",
        "        # callbacks=[lr_callback, checkpoint_callback],\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.fit(model_module) #, ckpt_path=\"last\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ef3132c26f4a4eb5aa6ab90cbd8f3330"
          ]
        },
        "id": "KNax96JP0CWR",
        "outputId": "942bb8f6-07e9-464b-c2d3-233ddb5c27e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef3132c26f4a4eb5aa6ab90cbd8f3330"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sameple #0, input_ids: tensor([57527, 18539, 48941, 37612, 48023, 57528, 57529, 57535, 50891, 39539,\n",
            "        15524, 57536, 57531, 54824, 35934, 57532, 57537, 50891, 39539, 55654,\n",
            "        57538, 57533, 50891, 39539, 55785, 57534, 57530,     2,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1])\n",
            "      Prediction: click the activate</s_prompt><s_target_bounding_box><s_xmax> 0.85</s_xmax><s_xmin> 0.15</s_xmin><s_ymax> 0.46</s_ymax><s_ymin> 0.39</s_ymin></s_target_bounding_box>\n",
            "          Answer: click the activate</s_prompt><s_target_bounding_box><s_xmax>0.98</s_xmax><s_xmin>0.02</s_xmin><s_ymax>0.46</s_ymax><s_ymin>0.39</s_ymin></s_target_bounding_box>\n",
            " Prediction bbox: {'xmin': 0.15, 'ymin': 0.39, 'xmax': 0.85, 'ymax': 0.46}\n",
            "     Answer bbox: {'xmin': 0.02, 'ymin': 0.39, 'xmax': 0.98, 'ymax': 0.46}\n",
            "Eval score (Center Distance): 0.0\n",
            "      Prediction: click on the option snow above ice</s_prompt><s_target_bounding_box><s_xmax> 0.94</s_xmax><s_xmin> 0.06</s_xmin><s_ymax> 0.88</s_ymax><s_ymin> 0.81</s_ymin></s_target_bounding_box>\n",
            "          Answer: click on the option snow above ice</s_prompt><s_target_bounding_box><s_xmax>0.84</s_xmax><s_xmin>0.06</s_xmin><s_ymax>0.86</s_ymax><s_ymin>0.82</s_ymin></s_target_bounding_box>\n",
            " Prediction bbox: {'xmin': 0.06, 'ymin': 0.81, 'xmax': 0.94, 'ymax': 0.88}\n",
            "     Answer bbox: {'xmin': 0.06, 'ymin': 0.82, 'xmax': 0.84, 'ymax': 0.86}\n",
            "Eval score (Center Distance): 0.050249378105604495\n",
            "      Prediction: select the arrow button which is to the immediate left of about</s_prompt><s_target_bounding_box><s_xmax> 0.12</s_xmax><s_xmin> 0.02</s_xmin><s_ymax> 0.11</s_ymax><s_ymin> 0.03</s_ymin></s_target_bounding_box>\n",
            "          Answer: select the arrow button which is to the immediate left of about</s_prompt><s_target_bounding_box><s_xmax>0.1</s_xmax><s_xmin>0.03</s_xmin><s_ymax>0.09</s_ymax><s_ymin>0.03</s_ymin></s_target_bounding_box>\n",
            " Prediction bbox: {'xmin': 0.02, 'ymin': 0.03, 'xmax': 0.12, 'ymax': 0.11}\n",
            "     Answer bbox: {'xmin': 0.03, 'ymin': 0.03, 'xmax': 0.1, 'ymax': 0.09}\n",
            "Eval score (Center Distance): 0.011180339887498952\n",
            "      Prediction: click on the text below audiobooks from audible</s_prompt><s_target_bounding_box><s_xmax> 0.98</s_xmax><s_xmin> 0.02</s_xmin><s_ymax> 0.44</s_ymax><s_ymin> 0.37</s_ymin></s_target_bounding_box>\n",
            "          Answer: click on the text below audiobooks from audible</s_prompt><s_target_bounding_box><s_xmax>0.98</s_xmax><s_xmin>0.18</s_xmin><s_ymax>0.42</s_ymax><s_ymin>0.38</s_ymin></s_target_bounding_box>\n",
            " Prediction bbox: {'xmin': 0.02, 'ymin': 0.37, 'xmax': 0.98, 'ymax': 0.44}\n",
            "     Answer bbox: {'xmin': 0.18, 'ymin': 0.38, 'xmax': 0.98, 'ymax': 0.42}\n",
            "Eval score (Center Distance): 0.08015609770940695\n",
            "      Prediction: go to the r below menu button</s_prompt><s_target_bounding_box><s_xmax> 0.2</s_xmax><s_xmin> 0.0</s_xmin><s_ymax> 0.17</s_ymax><s_ymin> 0.11</s_ymin></s_target_bounding_box>\n",
            "          Answer: go to the r below menu button</s_prompt><s_target_bounding_box><s_xmax>0.18</s_xmax><s_xmin>0.01</s_xmin><s_ymax>0.16</s_ymax><s_ymin>0.11</s_ymin></s_target_bounding_box>\n",
            " Prediction bbox: {'xmin': 0.0, 'ymin': 0.11, 'xmax': 0.2, 'ymax': 0.17}\n",
            "     Answer bbox: {'xmin': 0.01, 'ymin': 0.11, 'xmax': 0.18, 'ymax': 0.16}\n",
            "Eval score (Center Distance): 0.007071067811865481\n",
            "      Prediction: open the icon to the left of the create listing app</s_prompt><s_target_bounding_box><s_xmax> 0.14</s_xmax><s_xmin> 0.0</s_xmin><s_ymax> 0.11</s_ymax><s_ymin> 0.03</s_ymin></s_target_bounding_box>\n",
            "          Answer: open the icon to the left of the create listing app</s_prompt><s_target_bounding_box><s_xmax>0.14</s_xmax><s_xmin>0.0</s_xmin><s_ymax>0.11</s_ymax><s_ymin>0.03</s_ymin></s_target_bounding_box>\n",
            " Prediction bbox: {'xmin': 0.0, 'ymin': 0.03, 'xmax': 0.14, 'ymax': 0.11}\n",
            "     Answer bbox: {'xmin': 0.0, 'ymin': 0.03, 'xmax': 0.14, 'ymax': 0.11}\n",
            "Eval score (Center Distance): 0.0\n",
            "      Prediction: link contacts</s_prompt><s_target_bounding_box><s_xmax> 0.23</s_xmax><s_xmin> 0.0</s_xmin><s_ymax> 0.57</s_ymax><s_ymin> 0.54</s_ymin></s_target_bounding_box>\n",
            "          Answer: link contacts</s_prompt><s_target_bounding_box><s_xmax>0.91</s_xmax><s_xmin>0.03</s_xmin><s_ymax>0.58</s_ymax><s_ymin>0.54</s_ymin></s_target_bounding_box>\n",
            " Prediction bbox: {'xmin': 0.0, 'ymin': 0.54, 'xmax': 0.23, 'ymax': 0.57}\n",
            "     Answer bbox: {'xmin': 0.03, 'ymin': 0.54, 'xmax': 0.91, 'ymax': 0.58}\n",
            "Eval score (Center Distance): 0.35503520952153467\n",
            "      Prediction: select the tools item</s_prompt><s_target_bounding_box><s_xmax> 0.8</s_xmax><s_xmin> 0.6</s_xmin><s_ymax> 0.93</s_ymax><s_ymin> 0.9</s_ymin></s_target_bounding_box>\n",
            "          Answer: select the tools item</s_prompt><s_target_bounding_box><s_xmax>0.8</s_xmax><s_xmin>0.6</s_xmin><s_ymax>0.93</s_ymax><s_ymin>0.9</s_ymin></s_target_bounding_box>\n",
            " Prediction bbox: {'xmin': 0.6, 'ymin': 0.9, 'xmax': 0.8, 'ymax': 0.93}\n",
            "     Answer bbox: {'xmin': 0.6, 'ymin': 0.9, 'xmax': 0.8, 'ymax': 0.93}\n",
            "Eval score (Center Distance): 0.0\n",
            "      Prediction: click on the button that having a text call log at the bottom of the page</s_prompt><s_target_bounding_box><s_xmax> 0.98</s_xmax><s_xmin> 0.16</s_xmin><s_ymax> 0.92</s_ymax><s_ymin> 0.84</s_ymin></s_target_bounding_box>\n",
            "          Answer: click on the button that having a text call log at the bottom of the page</s_prompt><s_target_bounding_box><s_xmax>0.74</s_xmax><s_xmin>0.64</s_xmin><s_ymax>0.9</s_ymax><s_ymin>0.84</s_ymin></s_target_bounding_box>\n",
            " Prediction bbox: {'xmin': 0.16, 'ymin': 0.84, 'xmax': 0.98, 'ymax': 0.92}\n",
            "     Answer bbox: {'xmin': 0.64, 'ymin': 0.84, 'xmax': 0.74, 'ymax': 0.9}\n",
            "Eval score (Center Distance): 0.12041594578792295\n",
            "      Prediction: cute</s_prompt><s_target_bounding_box><s_xmax> 0.92</s_xmax><s_xmin> 0.83</s_xmin><s_ymax> 0.17</s_ymax><s_ymin> 0.11</s_ymin></s_target_bounding_box>\n",
            "          Answer: cute</s_prompt><s_target_bounding_box><s_xmax>0.88</s_xmax><s_xmin>0.8</s_xmin><s_ymax>0.17</s_ymax><s_ymin>0.11</s_ymin></s_target_bounding_box>\n",
            " Prediction bbox: {'xmin': 0.83, 'ymin': 0.11, 'xmax': 0.92, 'ymax': 0.17}\n",
            "     Answer bbox: {'xmin': 0.8, 'ymin': 0.11, 'xmax': 0.88, 'ymax': 0.17}\n",
            "Eval score (Center Distance): 0.03499999999999992\n",
            "      Prediction: click on the image on the top left</s_prompt><s_target_bounding_box><s_xmax> 0.14</s_xmax><s_xmin> 0.02</s_xmin><s_ymax> 0.11</s_ymax><s_ymin> 0.03</s_ymin></s_target_bounding_box>\n",
            "          Answer: click on the image on the top left</s_prompt><s_target_bounding_box><s_xmax>0.13</s_xmax><s_xmin>0.02</s_xmin><s_ymax>0.19</s_ymax><s_ymin>0.13</s_ymin></s_target_bounding_box>\n",
            " Prediction bbox: {'xmin': 0.02, 'ymin': 0.03, 'xmax': 0.14, 'ymax': 0.11}\n",
            "     Answer bbox: {'xmin': 0.02, 'ymin': 0.13, 'xmax': 0.13, 'ymax': 0.19}\n",
            "Eval score (Center Distance): 0.09013878188659974\n",
            "      Prediction: message settings</s_prompt><s_target_bounding_box><s_xmax> 0.41</s_xmax><s_xmin> 0.06</s_xmin><s_ymax> 0.27</s_ymax><s_ymin> 0.24</s_ymin></s_target_bounding_box>\n",
            "          Answer: message settings</s_prompt><s_target_bounding_box><s_xmax>0.4</s_xmax><s_xmin>0.06</s_xmin><s_ymax>0.27</s_ymax><s_ymin>0.24</s_ymin></s_target_bounding_box>\n",
            " Prediction bbox: {'xmin': 0.06, 'ymin': 0.24, 'xmax': 0.41, 'ymax': 0.27}\n",
            "     Answer bbox: {'xmin': 0.06, 'ymin': 0.24, 'xmax': 0.4, 'ymax': 0.27}\n",
            "Eval score (Center Distance): 0.004999999999999977\n",
            "      Prediction: click on exit continue</s_prompt><s_target_bounding_box><s_xmax> 0.82</s_xmax><s_xmin> 0.48</s_xmin><s_ymax> 0.92</s_ymax><s_ymin> 0.86</s_ymin></s_target_bounding_box>\n",
            "          Answer: click on exit  continue</s_prompt><s_target_bounding_box><s_xmax>0.75</s_xmax><s_xmin>0.43</s_xmin><s_ymax>0.92</s_ymax><s_ymin>0.86</s_ymin></s_target_bounding_box>\n",
            " Prediction bbox: {'xmin': 0.48, 'ymin': 0.86, 'xmax': 0.82, 'ymax': 0.92}\n",
            "     Answer bbox: {'xmin': 0.43, 'ymin': 0.86, 'xmax': 0.75, 'ymax': 0.92}\n",
            "Eval score (Center Distance): 0.05999999999999994\n",
            "      Prediction: select the icon above the settings icon in the bottom</s_prompt><s_target_bounding_box><s_xmax> 0.78</s_xmax><s_xmin> 0.17</s_xmin><s_ymax> 0.61</s_ymax><s_ymin> 0.56</s_ymin></s_target_bounding_box>\n",
            "          Answer: select the icon above the settings icon in the bottom</s_prompt><s_target_bounding_box><s_xmax>0.1</s_xmax><s_xmin>0.04</s_xmin><s_ymax>0.61</s_ymax><s_ymin>0.55</s_ymin></s_target_bounding_box>\n",
            " Prediction bbox: {'xmin': 0.17, 'ymin': 0.56, 'xmax': 0.78, 'ymax': 0.61}\n",
            "     Answer bbox: {'xmin': 0.04, 'ymin': 0.55, 'xmax': 0.1, 'ymax': 0.61}\n",
            "Eval score (Center Distance): 0.40503086302157276\n",
            "      Prediction: click on the icon above top shared at the bottom of the web page</s_prompt><s_target_bounding_box><s_xmax> 0.67</s_xmax><s_xmin> 0.33</s_xmin><s_ymax> 0.87</s_ymax><s_ymin> 0.81</s_ymin></s_target_bounding_box>\n",
            "          Answer: click on the icon above top shared at the bottom of the web page</s_prompt><s_target_bounding_box><s_xmax>0.53</s_xmax><s_xmin>0.47</s_xmin><s_ymax>0.9</s_ymax><s_ymin>0.87</s_ymin></s_target_bounding_box>\n",
            " Prediction bbox: {'xmin': 0.33, 'ymin': 0.81, 'xmax': 0.67, 'ymax': 0.87}\n",
            "     Answer bbox: {'xmin': 0.47, 'ymin': 0.87, 'xmax': 0.53, 'ymax': 0.9}\n",
            "Eval score (Center Distance): 0.04499999999999993\n",
            "      Prediction: select more</s_prompt><s_target_bounding_box><s_xmax> 0.98</s_xmax><s_xmin> 0.85</s_xmin><s_ymax> 0.58</s_ymax><s_ymin> 0.53</s_ymin></s_target_bounding_box>\n",
            "          Answer: select more</s_prompt><s_target_bounding_box><s_xmax>0.98</s_xmax><s_xmin>0.85</s_xmin><s_ymax>0.57</s_ymax><s_ymin>0.53</s_ymin></s_target_bounding_box>\n",
            " Prediction bbox: {'xmin': 0.85, 'ymin': 0.53, 'xmax': 0.98, 'ymax': 0.58}\n",
            "     Answer bbox: {'xmin': 0.85, 'ymin': 0.53, 'xmax': 0.98, 'ymax': 0.57}\n",
            "Eval score (Center Distance): 0.004999999999999893\n",
            "      Prediction: click the first download button</s_prompt><s_target_bounding_box><s_xmax> 0.96</s_xmax><s_xmin> 0.91</s_xmin><s_ymax> 0.52</s_ymax><s_ymin> 0.49</s_ymin></s_target_bounding_box>\n",
            "          Answer: click the first download button</s_prompt><s_target_bounding_box><s_xmax>0.21</s_xmax><s_xmin>0.04</s_xmin><s_ymax>0.59</s_ymax><s_ymin>0.49</s_ymin></s_target_bounding_box>\n",
            " Prediction bbox: {'xmin': 0.91, 'ymin': 0.49, 'xmax': 0.96, 'ymax': 0.52}\n",
            "     Answer bbox: {'xmin': 0.04, 'ymin': 0.49, 'xmax': 0.21, 'ymax': 0.59}\n",
            "Eval score (Center Distance): 0.8107558202072929\n",
            "      Prediction: select the blue color logo of notepad on the left</s_prompt><s_target_bounding_box><s_xmax> 0.27</s_xmax><s_xmin> 0.06</s_xmin><s_ymax> 0.93</s_ymax><s_ymin> 0.91</s_ymin></s_target_bounding_box>\n",
            "          Answer: select the blue color logo of notepad on the left</s_prompt><s_target_bounding_box><s_xmax>0.41</s_xmax><s_xmin>0.34</s_xmin><s_ymax>0.15</s_ymax><s_ymin>0.11</s_ymin></s_target_bounding_box>\n",
            " Prediction bbox: {'xmin': 0.06, 'ymin': 0.91, 'xmax': 0.27, 'ymax': 0.93}\n",
            "     Answer bbox: {'xmin': 0.34, 'ymin': 0.11, 'xmax': 0.41, 'ymax': 0.15}\n",
            "Eval score (Center Distance): 0.8174350127074324\n",
            "      Prediction: flip to sign in item</s_prompt><s_target_bounding_box><s_xmax> 0.22</s_xmax><s_xmin> 0.04</s_xmin><s_ymax> 0.09</s_ymax><s_ymin> 0.05</s_ymin></s_target_bounding_box>\n",
            "          Answer: flip to sign in item</s_prompt><s_target_bounding_box><s_xmax>0.22</s_xmax><s_xmin>0.02</s_xmin><s_ymax>0.11</s_ymax><s_ymin>0.05</s_ymin></s_target_bounding_box>\n",
            " Prediction bbox: {'xmin': 0.04, 'ymin': 0.05, 'xmax': 0.22, 'ymax': 0.09}\n",
            "     Answer bbox: {'xmin': 0.02, 'ymin': 0.05, 'xmax': 0.22, 'ymax': 0.11}\n",
            "Eval score (Center Distance): 0.014142135623730954\n",
            "      Prediction: additional information</s_prompt><s_target_bounding_box><s_xmax> 0.92</s_xmax><s_xmin> 0.08</s_xmin><s_ymax> 0.46</s_ymax><s_ymin> 0.42</s_ymin></s_target_bounding_box>\n",
            "          Answer: additional information</s_prompt><s_target_bounding_box><s_xmax>0.93</s_xmax><s_xmin>0.07</s_xmin><s_ymax>0.44</s_ymax><s_ymin>0.42</s_ymin></s_target_bounding_box>\n",
            " Prediction bbox: {'xmin': 0.08, 'ymin': 0.42, 'xmax': 0.92, 'ymax': 0.46}\n",
            "     Answer bbox: {'xmin': 0.07, 'ymin': 0.42, 'xmax': 0.93, 'ymax': 0.44}\n",
            "Eval score (Center Distance): 0.010000000000000009\n",
            "      Prediction: edit</s_prompt><s_target_bounding_box><s_xmax> 0.98</s_xmax><s_xmin> 0.86</s_xmin><s_ymax> 0.16</s_ymax><s_ymin> 0.13</s_ymin></s_target_bounding_box>\n",
            "          Answer: edit</s_prompt><s_target_bounding_box><s_xmax>0.98</s_xmax><s_xmin>0.8</s_xmin><s_ymax>0.17</s_ymax><s_ymin>0.12</s_ymin></s_target_bounding_box>\n",
            " Prediction bbox: {'xmin': 0.86, 'ymin': 0.13, 'xmax': 0.98, 'ymax': 0.16}\n",
            "     Answer bbox: {'xmin': 0.8, 'ymin': 0.12, 'xmax': 0.98, 'ymax': 0.17}\n",
            "Eval score (Center Distance): 0.029999999999999916\n",
            "      Prediction: select bottom right symbol</s_prompt><s_target_bounding_box><s_xmax> 0.98</s_xmax><s_xmin> 0.89</s_xmin><s_ymax> 0.92</s_ymax><s_ymin> 0.9</s_ymin></s_target_bounding_box>\n",
            "          Answer: select bottom right symbol</s_prompt><s_target_bounding_box><s_xmax>0.99</s_xmax><s_xmin>0.88</s_xmin><s_ymax>0.93</s_ymax><s_ymin>0.88</s_ymin></s_target_bounding_box>\n",
            " Prediction bbox: {'xmin': 0.89, 'ymin': 0.9, 'xmax': 0.98, 'ymax': 0.92}\n",
            "     Answer bbox: {'xmin': 0.88, 'ymin': 0.88, 'xmax': 0.99, 'ymax': 0.93}\n",
            "Eval score (Center Distance): 0.0050000000000000044\n",
            "      Prediction: your creators</s_prompt><s_target_bounding_box><s_xmax> 0.35</s_xmax><s_xmin> 0.15</s_xmin><s_ymax> 0.42</s_ymax><s_ymin> 0.39</s_ymin></s_target_bounding_box>\n",
            "          Answer: your creators</s_prompt><s_target_bounding_box><s_xmax>0.9</s_xmax><s_xmin>0.14</s_xmin><s_ymax>0.42</s_ymax><s_ymin>0.39</s_ymin></s_target_bounding_box>\n",
            " Prediction bbox: {'xmin': 0.15, 'ymin': 0.39, 'xmax': 0.35, 'ymax': 0.42}\n",
            "     Answer bbox: {'xmin': 0.14, 'ymin': 0.39, 'xmax': 0.9, 'ymax': 0.42}\n",
            "Eval score (Center Distance): 0.27\n",
            "      Prediction: click on terms and privacy</s_prompt><s_target_bounding_box><s_xmax> 0.73</s_xmax><s_xmin> 0.27</s_xmin><s_ymax> 0.91</s_ymax><s_ymin> 0.88</s_ymin></s_target_bounding_box>\n",
            "          Answer: click on terms and privacy</s_prompt><s_target_bounding_box><s_xmax>0.52</s_xmax><s_xmin>0.24</s_xmin><s_ymax>0.91</s_ymax><s_ymin>0.87</s_ymin></s_target_bounding_box>\n",
            " Prediction bbox: {'xmin': 0.27, 'ymin': 0.88, 'xmax': 0.73, 'ymax': 0.91}\n",
            "     Answer bbox: {'xmin': 0.24, 'ymin': 0.87, 'xmax': 0.52, 'ymax': 0.91}\n",
            "Eval score (Center Distance): 0.12010412149464314\n",
            "      Prediction: find the settings icon</s_prompt><s_target_bounding_box><s_xmax> 0.99</s_xmax><s_xmin> 0.01</s_xmin><s_ymax> 0.93</s_ymax><s_ymin> 0.11</s_ymin></s_target_bounding_box>\n",
            "          Answer: find the settings icon</s_prompt><s_target_bounding_box><s_xmax>0.86</s_xmax><s_xmin>0.71</s_xmin><s_ymax>0.09</s_ymax><s_ymin>0.03</s_ymin></s_target_bounding_box>\n",
            " Prediction bbox: {'xmin': 0.01, 'ymin': 0.11, 'xmax': 0.99, 'ymax': 0.93}\n",
            "     Answer bbox: {'xmin': 0.71, 'ymin': 0.03, 'xmax': 0.86, 'ymax': 0.09}\n",
            "Eval score (Center Distance): 0.5411330705103874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KuU1meNkN3-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Push to hub and reuse\n",
        "\n",
        "HuggingFace's [hub](https://huggingface.co/) is a nice place to host, version and share machine learning models (and datasets, and demos in the form of [Spaces](https://huggingface.co/spaces)).\n",
        "\n",
        "We first provide our authentication token."
      ],
      "metadata": {
        "id": "1xl4AeMl3jmb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pushing to the hub after training is as easy as:"
      ],
      "metadata": {
        "id": "7X7GV-YE5loA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_name = \"ivelin/donut-refexp-combined-v1\"\n",
        "\n",
        "#\n",
        "# here we push the processor and model to the hub\n",
        "# note that you can add `private=True` in case you're using the private hub\n",
        "# which makes sure the model is only shared with your colleagues\n",
        "model_module.processor.push_to_hub(repo_name) \n",
        "model_module.model.push_to_hub(repo_name)\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "gY24Xk8IDtNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reloading can then be done as:"
      ],
      "metadata": {
        "id": "lN_F7nY67cre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
        "\n",
        "processor = DonutProcessor.from_pretrained(repo_name)\n",
        "model = VisionEncoderDecoderModel.from_pretrained(repo_name)\n",
        "\n",
        "backup_repo_name = \"ivelin/donut-refexp-combined-v1-backup\"\n",
        "\n",
        "# save a backup in case uploading to the main model fails and corrupts the data\n",
        "model_module.processor.push_to_hub(backup_repo_name) \n",
        "model_module.model.push_to_hub(backup_repo_name)\n"
      ],
      "metadata": {
        "id": "chaFQM0R3mrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "\n",
        "For inference, you can use this [Gradio playground notebook](https://github.com/ivelin/donut_ui_refexp/blob/main/Donut_UI_RefExp_Gradio_Inference_Playground.ipynb) or this [Huggingface playspace](https://huggingface.co/spaces/ivelin/ui-refexp). Also see to the Donut [docs](https://huggingface.co/docs/transformers/main/en/model_doc/donut#inference)."
      ],
      "metadata": {
        "id": "9t50qDh-lGMg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Dph3FRU6p67k"
      }
    }
  ]
}